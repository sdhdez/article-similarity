{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a_path = get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX)\n",
    "docs_sample_a = load_pickle(docs_sample_a_path)\n",
    "# Load sample B\n",
    "docs_sample_b_path = get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX)\n",
    "docs_sample_b = load_pickle(docs_sample_b_path)\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 2000\n"
     ]
    }
   ],
   "source": [
    "docs_samples_content_path = get_relative_path(data_path, DOCS_SAMPLES_CONTENT)\n",
    "samples_content = load_pickle(docs_samples_content_path)\n",
    "print(\"Docs: %d\" % len(samples_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_path = get_relative_path(prefix_path, DEAULT_WORDVECTORS)\n",
    "os.path.exists(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load counts documents by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_samples_word_doc_count_path = get_relative_path(data_path, DOCS_SAMPLES_WORD_DOC_COUNT)\n",
    "samples_word_in_docs_count = load_pickle(docs_samples_word_doc_count_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure $idf(t, D) = log\\frac{N}{\\vert \\left\\{ d\\in D:t\\in d \\right\\} \\vert}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(docs_sample_a) + len(docs_sample_b)\n",
    "t_idf = {t: np.log(n_docs/v) for t, v in samples_word_in_docs_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65111072 0.70310882 0.65259718 ... 0.72468738 0.52235828 0.54822528]\n",
      " [0.70137091 0.80561423 0.71719402 ... 0.78705137 0.57018362 0.61170953]\n",
      " [0.68522791 0.69460987 0.69879183 ... 0.73962227 0.56383926 0.70678924]\n",
      " ...\n",
      " [0.65779476 0.75228731 0.68471401 ... 0.78790058 0.57673746 0.74272504]\n",
      " [0.59932566 0.66580782 0.49443457 ... 0.67596839 0.6500183  0.55437951]\n",
      " [0.71192622 0.68136118 0.65052729 ... 0.74431948 0.55242311 0.65809202]]\n"
     ]
    }
   ],
   "source": [
    "sample_a_indices = range(0, len(docs_sample_a))\n",
    "sample_b_indices = range(0, len(docs_sample_b))\n",
    "\n",
    "def get_content_for(index, samples_content, field='tokens'):\n",
    "    return samples_content[index][field]\n",
    "\n",
    "def tf_doc(terms):\n",
    "    tf = Counter(terms)\n",
    "    cnt = sum(tf.values())\n",
    "    return {k: v/cnt for k, v in tf.items()}\n",
    "\n",
    "m_measure_sim = np.zeros([len(docs_sample_a), len(docs_sample_b)]) \n",
    "for i, j in product(sample_a_indices, sample_a_indices):\n",
    "    # Term frequencies by document\n",
    "    tf_a = tf_doc(get_content_for(docs_sample_a[i], samples_content))\n",
    "    tf_b = tf_doc(get_content_for(docs_sample_b[j], samples_content))\n",
    "    # Term list\n",
    "    t_list = list(set(tf_a) | set(tf_b))\n",
    "    # TF-IDF\n",
    "    tf_idf_a = np.zeros([len(t_list),])\n",
    "    tf_idf_b = np.zeros([len(t_list),])\n",
    "    # wordvector length 300\n",
    "    centroid_a = mean = np.zeros(300, dtype=np.float64)\n",
    "    centroid_b = mean = np.zeros(300, dtype=np.float64)\n",
    "    for pos, t in enumerate(t_list):\n",
    "        if t and t in wordvectors.vocab and t in t_idf:\n",
    "            wv = wordvectors.get_vector(t)\n",
    "            if t in tf_a:\n",
    "                tf_idf_a[pos] = tf_a[t] * t_idf[t]\n",
    "                centroid_a += wv * tf_idf_a[pos]\n",
    "            if t in tf_b:\n",
    "                tf_idf_b[pos] = tf_b[t] * t_idf[t]\n",
    "                centroid_b += wv * tf_idf_b[pos]\n",
    "            \n",
    "    sum_tf_idf_a = sum(tf_idf_a)\n",
    "    sum_tf_idf_b = sum(tf_idf_b)\n",
    "    # Centroid\n",
    "    centroid_a /= sum_tf_idf_a\n",
    "    centroid_b /= sum_tf_idf_b\n",
    "    \n",
    "    # Measure\n",
    "    measure_sim = 1.0 - cosine_distance(centroid_a, centroid_b)\n",
    "    m_measure_sim[i, j] = measure_sim\n",
    "\n",
    "print(m_measure_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_samples_measure_sim_path = get_relative_path(data_path, DOCS_SAMPLES_WORD2VEC_TFIDF)\n",
    "save_pickle(m_measure_sim, docs_samples_measure_sim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66773104 0.72935469 0.63675897 ... 0.73157146 0.55387421 0.61209168]\n",
      " [0.71649824 0.78262252 0.68326415 ... 0.78500118 0.59432596 0.6567953 ]\n",
      " [0.69120158 0.75499128 0.65914085 ... 0.75728597 0.57334271 0.63360651]\n",
      " ...\n",
      " [0.70452175 0.76954075 0.67184318 ... 0.77187965 0.58439162 0.64581677]\n",
      " [0.59513725 0.65006135 0.56753237 ... 0.65203712 0.4936586  0.54554684]\n",
      " [0.7017605  0.76652467 0.66921001 ... 0.7688544  0.5821012  0.6432856 ]]\n"
     ]
    }
   ],
   "source": [
    "n_singularvalues = 1\n",
    "U, s, V = np.linalg.svd(m_measure_sim, full_matrices=False)\n",
    "D = np.diag(s[:n_singularvalues])\n",
    "m_udv_measure_sim = np.dot(U[:,:n_singularvalues], np.dot(D, V[:n_singularvalues,:]))\n",
    "print(m_udv_measure_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_samples_measure_sim_udv_path = get_relative_path(data_path, DOCS_SAMPLES_WORD2VEC_TFIDF_UDV)\n",
    "save_pickle(m_udv_measure_sim, docs_samples_measure_sim_udv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
