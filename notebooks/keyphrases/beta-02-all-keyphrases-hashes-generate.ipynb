{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default:  ~/kleis_data/corpus/semeval2017-task10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: SemEval 2017 Task 10 corpus doesn't exists.\n",
      "    - Download from here https://scienceie.github.io/resources.html\n",
      "    - Use one of the following paths.\n",
      "        + ./kleis_data/corpus/semeval2017-task10/\n",
      "        + ~/kleis_data/corpus/semeval2017-task10/\n",
      "        + /home/jupyterlab/.local/lib/python3.5/site-packages/kleis/kleis_data/corpus/semeval2017-task10/\n",
      "    - You can use pre-trained models.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import hashlib as hl\n",
    "import pickle\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "import somhos.methods.useful as mu\n",
    "import kleis.resources.dataset as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kleis = kl.load_corpus()\n",
    "kleis.training(features_method=\"simple-posseq\", filter_min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = \"resources/aminer/v1\"\n",
    "data_path = \"../../src/somhos/resources/aminer/v9beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample size: 1003 documents\n",
      " - Content from ../../src/somhos/resources/aminer/v9beta/sample-ids-100-10-0-related-fullcontent.bin\n",
      "Sample size: 1000 documents\n",
      " - Content from ../../src/somhos/resources/aminer/v9beta/sample-ids-100-10-0-random-fullcontent.bin\n"
     ]
    }
   ],
   "source": [
    "# Load document ids in the Same order than the matrices\n",
    "test_preselected = set(rd.get_sample_ids(data_path, related_docs=True))\n",
    "test_preselected = test_preselected | set(rd.get_sample_ids(data_path, related_docs=False))\n",
    "test_dataset = copy.deepcopy(test_preselected)\n",
    "train_dataset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_seed = 0\n",
    "random.seed(fixed_seed)\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../src/somhos/resources/aminer/v9beta/acm.01.txt', '../../src/somhos/resources/aminer/v9beta/acm.02.txt', '../../src/somhos/resources/aminer/v9beta/acm.03.txt']\n"
     ]
    }
   ],
   "source": [
    "# Read artminer\n",
    "datafiles = sorted(rd.get_filenames(data_path))\n",
    "print(datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/somhos/resources/aminer/v9beta/acm.02.txt\n"
     ]
    }
   ],
   "source": [
    "filepath = \"\"\n",
    "if datafiles:\n",
    "    filepath = datafiles[1]\n",
    "    print(filepath)\n",
    "\n",
    "dociter = None\n",
    "if Path(filepath).exists():\n",
    "    dociter = rd.get_aminer_txt(filepath, merge_text_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, elapsed_time = time.time(), 0\n",
    "kps_count = 0\n",
    "kps_hashes_counts = {}\n",
    "kps_hashes_keyphrases = {}\n",
    "kps_hashes_idocs = {}\n",
    "# idocs_wl_content = {} # idocs with large content\n",
    "\n",
    "kps_file_segment = 58\n",
    "tmp_data_path = data_path + \"/tmp-pickles\"\n",
    "if not Path(tmp_data_path).exists():\n",
    "    os.mkdir(tmp_data_path)\n",
    "kps_tmp_counts = tmp_data_path + \"/kps-tmp-counts-%d.pkl\"\n",
    "kps_tmp_keyphrases = tmp_data_path + \"/kps-tmp-keyphrases-%d.pkl\"\n",
    "kps_tmp_idocs = tmp_data_path + \"/kps-tmp-idocs-%d.pkl\"\n",
    "\n",
    "for i, (idoc, title, content) in enumerate(dociter):\n",
    "    # Check length of content\n",
    "    # idocs_wcontent[idoc] = True if content.split() > 50 else False \n",
    "    # Sampling test dataset\n",
    "    if random.random() <= threshold:\n",
    "        test_dataset.add(idoc)\n",
    "    else:\n",
    "        # Train dataset\n",
    "        train_dataset.add(idoc)\n",
    "    #if i > 10000:\n",
    "    #    break\n",
    "    if i <= 700000:\n",
    "        continue\n",
    "    if i % 100000 == 0:\n",
    "        print(\"Progress: %d\" % i, file=sys.stderr)\n",
    "        prev_elapsed_time = elapsed_time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Total time: %f\" % (elapsed_time/60/60), file=sys.stderr)\n",
    "        print(\"Elapsed time: %f\" % ((elapsed_time - prev_elapsed_time)/60/60), file=sys.stderr)\n",
    "        print(\"Hashes: %d\\n\" % len(kps_hashes_keyphrases), file=sys.stderr)\n",
    "        # Saving keywords\n",
    "        mu.save_pickle(kps_hashes_counts, kps_tmp_counts % kps_file_segment)\n",
    "        mu.save_pickle(kps_hashes_keyphrases, kps_tmp_keyphrases % kps_file_segment)\n",
    "        mu.save_pickle(kps_hashes_idocs, kps_tmp_idocs % kps_file_segment)\n",
    "        # increase no.\n",
    "        kps_file_segment += 1\n",
    "        # reset vars\n",
    "        del kps_hashes_counts\n",
    "        del kps_hashes_keyphrases\n",
    "        del kps_hashes_idocs\n",
    "        kps_hashes_counts = {}\n",
    "        kps_hashes_keyphrases = {}\n",
    "        kps_hashes_idocs = {}\n",
    "    # Avoid preselected documents\n",
    "    #if idoc in test_preselected:\n",
    "    #    print(\"Pre-selected\", idoc)\n",
    "    #    continue\n",
    "\n",
    "    try:\n",
    "        text = title.strip(\". \") + \". \" + content\n",
    "        keyphrases = kleis.label_text(text, post_processing=False)\n",
    "        for kpid, (kplabel, (kpstart, kpend)), kptext in keyphrases:\n",
    "            kps_count += 1\n",
    "            kplower = mu.lower_utf8(kptext)\n",
    "            kps_hash_16 = mu.hash_16bytes(kplower)\n",
    "            # count\n",
    "            kps_hashes_counts.setdefault(kps_hash_16, 0)\n",
    "            kps_hashes_counts[kps_hash_16] += 1\n",
    "            # normalized keyphrase\n",
    "            kps_hashes_keyphrases[kps_hash_16] = kplower\n",
    "            # id docs\n",
    "            kps_hashes_idocs.setdefault(kps_hash_16, set())\n",
    "            kps_hashes_idocs[kps_hash_16].add(idoc)\n",
    "    except ValueError:\n",
    "        pass\n",
    "        # print(\"\\nSkipped: %s\\n\" % idoc, file=sys.stderr)\n",
    "\n",
    "# save last hashes\n",
    "mu.save_pickle(kps_hashes_counts, kps_tmp_counts % kps_file_segment)\n",
    "mu.save_pickle(kps_hashes_keyphrases, kps_tmp_keyphrases % kps_file_segment)\n",
    "mu.save_pickle(kps_hashes_idocs, kps_tmp_idocs % kps_file_segment)\n",
    "# reset vars\n",
    "del kps_hashes_counts\n",
    "del kps_hashes_keyphrases\n",
    "del kps_hashes_idocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = data_path + \"/test-dataset-acm02.pkl\"\n",
    "if not Path(test_dataset_path).exists():\n",
    "    with open(test_dataset_path, \"wb\") as fout:\n",
    "        pickle.dump(test_dataset, fout, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "train_dataset_path = data_path + \"/train-dataset-acm02.pkl\"\n",
    "if not Path(train_dataset_path).exists():\n",
    "    with open(train_dataset_path, \"wb\") as fout:\n",
    "        pickle.dump(train_dataset, fout, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
