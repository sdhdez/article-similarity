{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import hashlib as hl\n",
    "import pickle\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "import somhos.methods.useful as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = \"resources/aminer/v1\"\n",
    "data_path = \"../../src/somhos/resources/aminer/v9beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data_path = data_path + \"/tmp-pickles\"\n",
    "if not Path(tmp_data_path).exists():\n",
    "    os.mkdir(tmp_data_path)\n",
    "kps_tmp_counts = tmp_data_path + \"/kps-tmp-counts-%d.pkl\"\n",
    "kps_tmp_keyphrases = tmp_data_path + \"/kps-tmp-keyphrases-%d.pkl\"\n",
    "kps_tmp_idocs = tmp_data_path + \"/kps-tmp-idocs-%d.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tmp_train = data_path + \"/train-dataset-acm0%d.pkl\"\n",
    "docs_tmp_test = data_path + \"/test-dataset-acm0%d.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps_file_segment = 150\n",
    "kps_hashes_counts = {}\n",
    "kps_hashes_keyphrases = {}\n",
    "kps_hashes_idocs = {}\n",
    "for i in range(0, kps_file_segment + 1):\n",
    "    if Path(kps_tmp_counts % i).exists():\n",
    "        kps_hashes_counts.update(mu.load_pickle(kps_tmp_counts % i))\n",
    "        kps_hashes_keyphrases.update(mu.load_pickle(kps_tmp_keyphrases % i))\n",
    "        kps_hashes_idocs.update(mu.load_pickle(kps_tmp_idocs % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../src/somhos/resources/aminer/v9beta/train-dataset-acm01.pkl\n",
      "../../src/somhos/resources/aminer/v9beta/train-dataset-acm02.pkl\n",
      "../../src/somhos/resources/aminer/v9beta/train-dataset-acm03.pkl\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "train_dataset = []\n",
    "# acm\n",
    "for i in range(1, 4):\n",
    "    if Path(docs_tmp_train % i).exists():\n",
    "        print(docs_tmp_train % i)\n",
    "        train_dataset += mu.load_pickle(docs_tmp_train % i)\n",
    "        test_dataset += mu.load_pickle(docs_tmp_test % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tmp files: 150\n",
      " - Test dataset: 243520\n",
      " - Train dataset: 2146777\n",
      " - Keyphrases hashes-counts: 3922202\n",
      " - Keyphrases normalized hashes: 3922202\n",
      " - Keyphrases-docsids: 3922202\n"
     ]
    }
   ],
   "source": [
    "print(\" - Tmp files: %d\" % (kps_file_segment))\n",
    "print(\" - Test dataset: %d\" % len(test_dataset))\n",
    "print(\" - Train dataset: %d\" % len(train_dataset))\n",
    "# print(\" - Keyphrases: %d\" % kps_count)\n",
    "print(\" - Keyphrases hashes-counts: %d\" % len(kps_hashes_counts))\n",
    "print(\" - Keyphrases normalized hashes: %d\" % len(kps_hashes_keyphrases))\n",
    "print(\" - Keyphrases-docsids: %d\" % len(kps_hashes_idocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps_directory = {}\n",
    "kps_normalized = []\n",
    "kps_counts = []\n",
    "kps_docs_counts = []\n",
    "kps_docs_ids = []\n",
    "for i, (k, v) in enumerate(sorted(kps_hashes_counts.items(), key=lambda x: x[1], reverse=True)):\n",
    "    kps_normalized.append(kps_hashes_keyphrases[k])\n",
    "    kps_counts.append(kps_hashes_counts[k])\n",
    "    kps_docs_counts.append(len(kps_hashes_idocs[k]))\n",
    "    kps_docs_ids.append(kps_hashes_idocs[k])\n",
    "    kps_directory[k] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps_directory_inverse = {}\n",
    "for k, v in kps_directory.items():\n",
    "    for idoc in kps_hashes_idocs[k]: \n",
    "        kps_directory_inverse.setdefault(idoc, set())\n",
    "        kps_directory_inverse[idoc].add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d \" % len(kps_normalized))\n",
    "print(\"%d \" % len(kps_counts))\n",
    "print(\"%d \" % len(kps_docs_counts))\n",
    "print(\"%d \" % len(kps_docs_ids))\n",
    "print(\"%d \" % len(kps_directory))\n",
    "print(\"%d \" % len(kps_directory_inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243520 \n",
      "2146777 \n",
      "2385066 == 2390297\n"
     ]
    }
   ],
   "source": [
    "print(\"%d \" % len(test_dataset))\n",
    "print(\"%d \" % len(train_dataset))\n",
    "print(\"2385066 == %d\" % (len(train_dataset) + len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'literature', b'call', b'example', b'n', b'q']\n",
      "[423, 422, 420, 420, 418]\n",
      "[396, 418, 395, 256, 184]\n",
      "[100, 101, 102, 103, 104]\n",
      "[('index2358587', {0, 769, 1184, 99, 2465, 2824, 11209, 2602, 13291, 3906536, 14, 16722, 3962}), ('index2335651', {0, 159904, 642, 64461, 16, 22, 121, 1147, 39964}), ('index2367736', {0, 8425, 3979, 169422, 324242, 3909106, 127}), ('index2351487', {0, 3918852, 38, 103, 84843, 27628, 14, 16, 17, 80, 5459, 20, 117, 22}), ('index2355729', {0, 529, 2101, 13115, 892}), ('index2366313', {0, 6464, 526, 17, 529, 20, 21, 3898358, 2231, 952, 3899060, 3899773}), ('index2355310', {0, 1, 484, 1956, 25128, 42, 80, 17, 3600, 4530, 20, 229456, 229841, 560372, 1058362, 716570}), ('index2341375', {0, 193, 352, 1800, 1066219, 10189}), ('index2336240', {0, 602048, 3, 1092, 8, 2120, 17, 53, 158}), ('index2355730', {0, 739, 1956, 2307, 1367278, 603, 158})]\n"
     ]
    }
   ],
   "source": [
    "seg_start = 100\n",
    "seg_end = seg_start + 5\n",
    "print(kps_normalized[seg_start:seg_end])\n",
    "print(kps_counts[seg_start:seg_end])\n",
    "print(kps_docs_counts[seg_start:seg_end])\n",
    "# print(kps_docs_ids[seg_start:seg_end])\n",
    "print([kps_directory[mu.hash_16bytes(kn)] for kn in kps_normalized[seg_start:seg_end]])\n",
    "print([di for i, di in enumerate(kps_directory_inverse.items()) if i < 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps_normalized_path = data_path + \"/kps-normalized-simpseq10-nopost.pkl\"\n",
    "if not Path(kps_normalized_path).exists():\n",
    "    with open(kps_normalized_path, \"wb\") as fout:\n",
    "        pickle.dump(kps_normalized, fout, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "kps_counts_path = data_path + \"/kps-counts-simpseq10-nopost.pkl\"\n",
    "if not Path(kps_counts_path).exists():\n",
    "    with open(kps_counts_path, \"wb\") as fout:\n",
    "        pickle.dump(kps_counts, fout, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "kps_docs_counts_path = data_path + \"/kps-docs-counts-simpseq10-nopost.pkl\"\n",
    "if not Path(kps_docs_counts_path).exists():\n",
    "    with open(kps_docs_counts_path, \"wb\") as fout:\n",
    "        pickle.dump(kps_docs_counts, fout, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "kps_docs_ids_path = data_path + \"/kps-docs-ids-simpseq10-nopost.pkl\"\n",
    "if not Path(kps_docs_ids_path).exists():\n",
    "    with open(kps_docs_ids_path, \"wb\") as fout:\n",
    "        pickle.dump(kps_docs_ids, fout, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "kps_directory_path = data_path + \"/kps-directory-simpseq10-nopost.pkl\"\n",
    "if not Path(kps_directory_path).exists():\n",
    "    with open(kps_directory_path, \"wb\") as fout:\n",
    "        pickle.dump(kps_directory, fout, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "kps_directory_inverse_path = data_path + \"/kps-directory-inverse-simpseq10-nopost.pkl\"\n",
    "if not Path(kps_directory_inverse_path).exists():\n",
    "    with open(kps_directory_inverse_path, \"wb\") as fout:\n",
    "        pickle.dump(kps_directory_inverse, fout, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = data_path + \"/test-dataset.pkl\"\n",
    "if not Path(test_dataset_path).exists():\n",
    "    with open(test_dataset_path, \"wb\") as fout:\n",
    "        pickle.dump(test_dataset, fout, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "train_dataset_path = data_path + \"/train-dataset.pkl\"\n",
    "if not Path(train_dataset_path).exists():\n",
    "    with open(train_dataset_path, \"wb\") as fout:\n",
    "        pickle.dump(train_dataset, fout, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
