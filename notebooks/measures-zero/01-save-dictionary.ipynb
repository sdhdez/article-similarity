{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "import somhos.methods.useful as mu\n",
    "from somhos.methods.useful import save_pickle, load_pickle\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_content = load_pickle(get_relative_path(data_path, DOCS_SAMPLES_CONTENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Lossless Condensers, Unbalanced Expanders, And Extractors',\n",
       " 'text': 'Lossless Condensers, Unbalanced Expanders, And Extractors Trevisan showed that many pseudorandom generator constructions give rise to constructions of explicit extractors. We show how to use such constructions to obtain explicit lossless condensers. A lossless condenser is a probabilistic map using only O(logn) additional random bits that maps n bits strings to poly(logK) bit strings, such that any source with support size K is mapped almost injectively to the smaller domain. Our construction remains the best lossless condenser to date. By composing our condenser with previous extractors, we obtain new, improved extractors. For small enough min-entropies our extractors can output all of the randomness with only O(logn) bits. We also obtain a new disperser that works for every entropy loss, uses an O(logn) bit seed, and has only O(logn) entropy loss. This is the best disperser construction to date, and yields other applications. Finally, our lossless condenser can be viewed as an unbalanced bipartite graph with strong expansion properties.',\n",
       " 'content': 'Trevisan showed that many pseudorandom generator constructions give rise to constructions of explicit extractors. We show how to use such constructions to obtain explicit lossless condensers. A lossless condenser is a probabilistic map using only O(logn) additional random bits that maps n bits strings to poly(logK) bit strings, such that any source with support size K is mapped almost injectively to the smaller domain. Our construction remains the best lossless condenser to date. By composing our condenser with previous extractors, we obtain new, improved extractors. For small enough min-entropies our extractors can output all of the randomness with only O(logn) bits. We also obtain a new disperser that works for every entropy loss, uses an O(logn) bit seed, and has only O(logn) entropy loss. This is the best disperser construction to date, and yields other applications. Finally, our lossless condenser can be viewed as an unbalanced bipartite graph with strong expansion properties.',\n",
       " 'tokens': ['lossless',\n",
       "  'condensers',\n",
       "  'unbalanced',\n",
       "  'expanders',\n",
       "  'extractors',\n",
       "  'trevisan',\n",
       "  'showed',\n",
       "  'many',\n",
       "  'pseudorandom',\n",
       "  'generator',\n",
       "  'constructions',\n",
       "  'give',\n",
       "  'rise',\n",
       "  'constructions',\n",
       "  'explicit',\n",
       "  'extractors',\n",
       "  'show',\n",
       "  'how',\n",
       "  'use',\n",
       "  'such',\n",
       "  'constructions',\n",
       "  'obtain',\n",
       "  'explicit',\n",
       "  'lossless',\n",
       "  'condensers',\n",
       "  'lossless',\n",
       "  'condenser',\n",
       "  'probabilistic',\n",
       "  'map',\n",
       "  'using',\n",
       "  'only',\n",
       "  'logn',\n",
       "  'additional',\n",
       "  'random',\n",
       "  'bits',\n",
       "  'maps',\n",
       "  'bits',\n",
       "  'strings',\n",
       "  'poly',\n",
       "  'logk',\n",
       "  'bit',\n",
       "  'strings',\n",
       "  'such',\n",
       "  'any',\n",
       "  'source',\n",
       "  'support',\n",
       "  'size',\n",
       "  'mapped',\n",
       "  'almost',\n",
       "  'injectively',\n",
       "  'smaller',\n",
       "  'domain',\n",
       "  'our',\n",
       "  'construction',\n",
       "  'remains',\n",
       "  'best',\n",
       "  'lossless',\n",
       "  'condenser',\n",
       "  'date',\n",
       "  'composing',\n",
       "  'our',\n",
       "  'condenser',\n",
       "  'previous',\n",
       "  'extractors',\n",
       "  'obtain',\n",
       "  'new',\n",
       "  'improved',\n",
       "  'extractors',\n",
       "  'small',\n",
       "  'enough',\n",
       "  'min',\n",
       "  'entropies',\n",
       "  'our',\n",
       "  'extractors',\n",
       "  'output',\n",
       "  'all',\n",
       "  'randomness',\n",
       "  'only',\n",
       "  'logn',\n",
       "  'bits',\n",
       "  'also',\n",
       "  'obtain',\n",
       "  'new',\n",
       "  'disperser',\n",
       "  'works',\n",
       "  'every',\n",
       "  'entropy',\n",
       "  'loss',\n",
       "  'uses',\n",
       "  'logn',\n",
       "  'bit',\n",
       "  'seed',\n",
       "  'has',\n",
       "  'only',\n",
       "  'logn',\n",
       "  'entropy',\n",
       "  'loss',\n",
       "  'best',\n",
       "  'disperser',\n",
       "  'construction',\n",
       "  'date',\n",
       "  'yields',\n",
       "  'other',\n",
       "  'applications',\n",
       "  'finally',\n",
       "  'our',\n",
       "  'lossless',\n",
       "  'condenser',\n",
       "  'viewed',\n",
       "  'unbalanced',\n",
       "  'bipartite',\n",
       "  'graph',\n",
       "  'strong',\n",
       "  'expansion',\n",
       "  'properties'],\n",
       " 'kps-normalized': [b'unbalanced expanders',\n",
       "  b'unbalanced expanders',\n",
       "  b'explicit extractors',\n",
       "  b'logn',\n",
       "  b'logk',\n",
       "  b'source',\n",
       "  b'domain',\n",
       "  b'construction',\n",
       "  b'date',\n",
       "  b'improved extractors',\n",
       "  b'small',\n",
       "  b'extractors',\n",
       "  b'randomness',\n",
       "  b'logn',\n",
       "  b'o',\n",
       "  b'logn',\n",
       "  b'logn',\n",
       "  b'date',\n",
       "  b'unbalanced bipartite graph',\n",
       "  b'strong expansion properties'],\n",
       " 'bag-of-words': {'additional',\n",
       "  'all',\n",
       "  'almost',\n",
       "  'also',\n",
       "  'any',\n",
       "  'applications',\n",
       "  'best',\n",
       "  'bipartite',\n",
       "  'bit',\n",
       "  'bits',\n",
       "  'composing',\n",
       "  'condenser',\n",
       "  'condensers',\n",
       "  'construction',\n",
       "  'constructions',\n",
       "  'date',\n",
       "  'disperser',\n",
       "  'domain',\n",
       "  'enough',\n",
       "  'entropies',\n",
       "  'entropy',\n",
       "  'every',\n",
       "  'expanders',\n",
       "  'expansion',\n",
       "  'explicit',\n",
       "  'extractors',\n",
       "  'finally',\n",
       "  'generator',\n",
       "  'give',\n",
       "  'graph',\n",
       "  'has',\n",
       "  'how',\n",
       "  'improved',\n",
       "  'injectively',\n",
       "  'logk',\n",
       "  'logn',\n",
       "  'loss',\n",
       "  'lossless',\n",
       "  'many',\n",
       "  'map',\n",
       "  'mapped',\n",
       "  'maps',\n",
       "  'min',\n",
       "  'new',\n",
       "  'obtain',\n",
       "  'only',\n",
       "  'other',\n",
       "  'our',\n",
       "  'output',\n",
       "  'poly',\n",
       "  'previous',\n",
       "  'probabilistic',\n",
       "  'properties',\n",
       "  'pseudorandom',\n",
       "  'random',\n",
       "  'randomness',\n",
       "  'remains',\n",
       "  'rise',\n",
       "  'seed',\n",
       "  'show',\n",
       "  'showed',\n",
       "  'size',\n",
       "  'small',\n",
       "  'smaller',\n",
       "  'source',\n",
       "  'strings',\n",
       "  'strong',\n",
       "  'such',\n",
       "  'support',\n",
       "  'trevisan',\n",
       "  'unbalanced',\n",
       "  'use',\n",
       "  'uses',\n",
       "  'using',\n",
       "  'viewed',\n",
       "  'works',\n",
       "  'yields'},\n",
       " 'bag-of-kps': {b'construction',\n",
       "  b'date',\n",
       "  b'domain',\n",
       "  b'explicit extractors',\n",
       "  b'extractors',\n",
       "  b'improved extractors',\n",
       "  b'logk',\n",
       "  b'logn',\n",
       "  b'o',\n",
       "  b'randomness',\n",
       "  b'small',\n",
       "  b'source',\n",
       "  b'strong expansion properties',\n",
       "  b'unbalanced bipartite graph',\n",
       "  b'unbalanced expanders'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_content['index980808']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 index980808\n",
      "10000 1723\n",
      "\tCorpus tokens\n",
      "10000 [12810, 29096, 13409, 29097, 29098, 29099, 6782, 628, 29100, 2760, 5001, 145, 2369, 5001, 6748, 29098, 1501, 51, 269, 36, 5001, 1013, 6748, 12810, 29096, 12810, 29101, 1014, 3413, 418, 175, 2662, 334, 1158, 7518, 2289, 7518, 11428, 15291, 21731, 1836, 11428, 36, 704, 439, 333, 6, 6780, 972, 29102, 2580, 465, 205, 1640, 5519, 1302, 12810, 29101, 369, 11848, 205, 29101, 935, 29098, 1013, 362, 1283, 29098, 262, 2938, 3557, 29103, 205, 29098, 1089, 15, 27921, 175, 2662, 7518, 27, 1013, 362, 29104, 957, 1320, 1037, 1765, 254, 2662, 1836, 4312, 193, 175, 2662, 1037, 1765, 1302, 29104, 1640, 369, 4229, 670, 823, 169, 205, 12810, 29101, 3741, 13409, 3680, 2281, 6166, 2611, 124]\n",
      "10000 [(12810, 5), (29096, 2), (13409, 2), (29097, 1), (29098, 5), (29099, 1), (6782, 1), (628, 1), (29100, 1), (2760, 1), (5001, 3), (145, 1), (2369, 1), (6748, 2), (1501, 1), (51, 1), (269, 1), (36, 2), (1013, 3), (29101, 4), (1014, 1), (3413, 1), (418, 1), (175, 3), (2662, 4), (334, 1), (1158, 1), (7518, 3), (2289, 1), (11428, 2), (15291, 1), (21731, 1), (1836, 2), (704, 1), (439, 1), (333, 1), (6, 1), (6780, 1), (972, 1), (29102, 1), (2580, 1), (465, 1), (205, 4), (1640, 2), (5519, 1), (1302, 2), (369, 2), (11848, 1), (935, 1), (362, 2), (1283, 1), (262, 1), (2938, 1), (3557, 1), (29103, 1), (1089, 1), (15, 1), (27921, 1), (27, 1), (29104, 2), (957, 1), (1320, 1), (1037, 2), (1765, 2), (254, 1), (4312, 1), (193, 1), (4229, 1), (670, 1), (823, 1), (169, 1), (3741, 1), (3680, 1), (2281, 1), (6166, 1), (2611, 1), (124, 1)]\n",
      "\tCorpus keyphrases\n",
      "10000 [27101, 27101, 27102, 1123, 27103, 1315, 582, 574, 114, 27104, 2111, 27105, 25305, 1123, 401, 1123, 1123, 114, 27106, 27107]\n",
      "10000 [(27101, 2), (27102, 1), (1123, 4), (27103, 1), (1315, 1), (582, 1), (574, 1), (114, 2), (27104, 1), (2111, 1), (27105, 1), (25305, 1), (401, 1), (27106, 1), (27107, 1)]\n"
     ]
    }
   ],
   "source": [
    "doc_directory = {}\n",
    "doc_inverse_directory = []\n",
    "unique_tokens_counts = Counter()\n",
    "unique_keyphrases_counts = Counter()\n",
    "for i, (k, doc_content) in enumerate(samples_content.items()):\n",
    "    doc_directory[k] = i\n",
    "    doc_inverse_directory.append(k)\n",
    "    unique_tokens_counts.update(doc_content['tokens'])\n",
    "    unique_keyphrases_counts.update(doc_content['kps-normalized'])\n",
    "\n",
    "dictionary_tokens = {k:i for i, k in enumerate(unique_tokens_counts.keys())}\n",
    "dictionary_keyphrases = {k:i for i, k in enumerate(unique_keyphrases_counts.keys())}\n",
    "\n",
    "corpus_tokens = []\n",
    "corpus_bag_of_words = []\n",
    "corpus_keyphrases = []\n",
    "corpus_bag_of_keyphrases = []\n",
    "for i, docid in enumerate(doc_inverse_directory):\n",
    "    doc_content = samples_content[doc_inverse_directory[i]]\n",
    "    # Corpus tokens\n",
    "    corpus_tokens.append([dictionary_tokens[token] for token in doc_content['tokens']])\n",
    "    corpus_bag_of_words.append([(dictionary_tokens[k], c) for k, c in Counter(doc_content['tokens']).items()])\n",
    "    # Corpus keyphrases\n",
    "    corpus_keyphrases.append([dictionary_keyphrases[k] for k in doc_content['kps-normalized']])\n",
    "    corpus_bag_of_keyphrases.append([(dictionary_keyphrases[k], c) for k, c in Counter(doc_content['kps-normalized']).items()])\n",
    "\n",
    "example = 1723\n",
    "print(len(doc_inverse_directory), doc_inverse_directory[example])\n",
    "print(len(doc_directory), doc_directory[doc_inverse_directory[example]])\n",
    "print(\"\\tCorpus tokens\")\n",
    "print(len(corpus_tokens), corpus_tokens[example])\n",
    "print(len(corpus_bag_of_words), corpus_bag_of_words[example])\n",
    "print(\"\\tCorpus keyphrases\")\n",
    "print(len(corpus_keyphrases), corpus_keyphrases[example])\n",
    "print(len(corpus_bag_of_keyphrases), corpus_bag_of_keyphrases[example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(doc_directory, get_relative_path(data_path, DOC_DIRECTORY))\n",
    "save_pickle(doc_inverse_directory, get_relative_path(data_path, DOC_INVERSE_DIRECTORY))\n",
    "save_pickle(unique_tokens_counts, get_relative_path(data_path, UNIQUE_TOKENS_COUNTS))\n",
    "save_pickle(unique_keyphrases_counts, get_relative_path(data_path, UNIQUE_KEYPHRASES_COUNTS))\n",
    "save_pickle(dictionary_tokens, get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "save_pickle(dictionary_keyphrases, get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "save_pickle(corpus_tokens, get_relative_path(data_path, CORPUS_TOKENS))\n",
    "save_pickle(corpus_bag_of_words, get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "save_pickle(corpus_keyphrases, get_relative_path(data_path, CORPUS_KEYPHRASES))\n",
    "save_pickle(corpus_bag_of_keyphrases, get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
