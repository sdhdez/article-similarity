{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "import somhos.methods.useful as mu\n",
    "from somhos.methods.useful import save_pickle, load_pickle\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_content = load_pickle(get_relative_path(data_path, DOCS_SAMPLES_CONTENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Lossless Condensers, Unbalanced Expanders, And Extractors',\n",
       " 'text': 'Lossless Condensers, Unbalanced Expanders, And Extractors Trevisan showed that many pseudorandom generator constructions give rise to constructions of explicit extractors. We show how to use such constructions to obtain explicit lossless condensers. A lossless condenser is a probabilistic map using only O(logn) additional random bits that maps n bits strings to poly(logK) bit strings, such that any source with support size K is mapped almost injectively to the smaller domain. Our construction remains the best lossless condenser to date. By composing our condenser with previous extractors, we obtain new, improved extractors. For small enough min-entropies our extractors can output all of the randomness with only O(logn) bits. We also obtain a new disperser that works for every entropy loss, uses an O(logn) bit seed, and has only O(logn) entropy loss. This is the best disperser construction to date, and yields other applications. Finally, our lossless condenser can be viewed as an unbalanced bipartite graph with strong expansion properties.',\n",
       " 'content': 'Trevisan showed that many pseudorandom generator constructions give rise to constructions of explicit extractors. We show how to use such constructions to obtain explicit lossless condensers. A lossless condenser is a probabilistic map using only O(logn) additional random bits that maps n bits strings to poly(logK) bit strings, such that any source with support size K is mapped almost injectively to the smaller domain. Our construction remains the best lossless condenser to date. By composing our condenser with previous extractors, we obtain new, improved extractors. For small enough min-entropies our extractors can output all of the randomness with only O(logn) bits. We also obtain a new disperser that works for every entropy loss, uses an O(logn) bit seed, and has only O(logn) entropy loss. This is the best disperser construction to date, and yields other applications. Finally, our lossless condenser can be viewed as an unbalanced bipartite graph with strong expansion properties.',\n",
       " 'tokens': ['lossless',\n",
       "  'condensers',\n",
       "  'unbalanced',\n",
       "  'expanders',\n",
       "  'extractors',\n",
       "  'trevisan',\n",
       "  'showed',\n",
       "  'many',\n",
       "  'pseudorandom',\n",
       "  'generator',\n",
       "  'constructions',\n",
       "  'give',\n",
       "  'rise',\n",
       "  'constructions',\n",
       "  'explicit',\n",
       "  'extractors',\n",
       "  'show',\n",
       "  'how',\n",
       "  'use',\n",
       "  'such',\n",
       "  'constructions',\n",
       "  'obtain',\n",
       "  'explicit',\n",
       "  'lossless',\n",
       "  'condensers',\n",
       "  'lossless',\n",
       "  'condenser',\n",
       "  'probabilistic',\n",
       "  'map',\n",
       "  'using',\n",
       "  'only',\n",
       "  'logn',\n",
       "  'additional',\n",
       "  'random',\n",
       "  'bits',\n",
       "  'maps',\n",
       "  'bits',\n",
       "  'strings',\n",
       "  'poly',\n",
       "  'logk',\n",
       "  'bit',\n",
       "  'strings',\n",
       "  'such',\n",
       "  'any',\n",
       "  'source',\n",
       "  'support',\n",
       "  'size',\n",
       "  'mapped',\n",
       "  'almost',\n",
       "  'injectively',\n",
       "  'smaller',\n",
       "  'domain',\n",
       "  'our',\n",
       "  'construction',\n",
       "  'remains',\n",
       "  'best',\n",
       "  'lossless',\n",
       "  'condenser',\n",
       "  'date',\n",
       "  'composing',\n",
       "  'our',\n",
       "  'condenser',\n",
       "  'previous',\n",
       "  'extractors',\n",
       "  'obtain',\n",
       "  'new',\n",
       "  'improved',\n",
       "  'extractors',\n",
       "  'small',\n",
       "  'enough',\n",
       "  'min',\n",
       "  'entropies',\n",
       "  'our',\n",
       "  'extractors',\n",
       "  'output',\n",
       "  'all',\n",
       "  'randomness',\n",
       "  'only',\n",
       "  'logn',\n",
       "  'bits',\n",
       "  'also',\n",
       "  'obtain',\n",
       "  'new',\n",
       "  'disperser',\n",
       "  'works',\n",
       "  'every',\n",
       "  'entropy',\n",
       "  'loss',\n",
       "  'uses',\n",
       "  'logn',\n",
       "  'bit',\n",
       "  'seed',\n",
       "  'has',\n",
       "  'only',\n",
       "  'logn',\n",
       "  'entropy',\n",
       "  'loss',\n",
       "  'best',\n",
       "  'disperser',\n",
       "  'construction',\n",
       "  'date',\n",
       "  'yields',\n",
       "  'other',\n",
       "  'applications',\n",
       "  'finally',\n",
       "  'our',\n",
       "  'lossless',\n",
       "  'condenser',\n",
       "  'viewed',\n",
       "  'unbalanced',\n",
       "  'bipartite',\n",
       "  'graph',\n",
       "  'strong',\n",
       "  'expansion',\n",
       "  'properties'],\n",
       " 'kps-normalized': [b'unbalanced expanders',\n",
       "  b'unbalanced expanders',\n",
       "  b'explicit extractors',\n",
       "  b'logn',\n",
       "  b'logk',\n",
       "  b'source',\n",
       "  b'domain',\n",
       "  b'construction',\n",
       "  b'date',\n",
       "  b'improved extractors',\n",
       "  b'small',\n",
       "  b'extractors',\n",
       "  b'randomness',\n",
       "  b'logn',\n",
       "  b'o',\n",
       "  b'logn',\n",
       "  b'logn',\n",
       "  b'date',\n",
       "  b'unbalanced bipartite graph',\n",
       "  b'strong expansion properties'],\n",
       " 'bag-of-words': {'additional',\n",
       "  'all',\n",
       "  'almost',\n",
       "  'also',\n",
       "  'any',\n",
       "  'applications',\n",
       "  'best',\n",
       "  'bipartite',\n",
       "  'bit',\n",
       "  'bits',\n",
       "  'composing',\n",
       "  'condenser',\n",
       "  'condensers',\n",
       "  'construction',\n",
       "  'constructions',\n",
       "  'date',\n",
       "  'disperser',\n",
       "  'domain',\n",
       "  'enough',\n",
       "  'entropies',\n",
       "  'entropy',\n",
       "  'every',\n",
       "  'expanders',\n",
       "  'expansion',\n",
       "  'explicit',\n",
       "  'extractors',\n",
       "  'finally',\n",
       "  'generator',\n",
       "  'give',\n",
       "  'graph',\n",
       "  'has',\n",
       "  'how',\n",
       "  'improved',\n",
       "  'injectively',\n",
       "  'logk',\n",
       "  'logn',\n",
       "  'loss',\n",
       "  'lossless',\n",
       "  'many',\n",
       "  'map',\n",
       "  'mapped',\n",
       "  'maps',\n",
       "  'min',\n",
       "  'new',\n",
       "  'obtain',\n",
       "  'only',\n",
       "  'other',\n",
       "  'our',\n",
       "  'output',\n",
       "  'poly',\n",
       "  'previous',\n",
       "  'probabilistic',\n",
       "  'properties',\n",
       "  'pseudorandom',\n",
       "  'random',\n",
       "  'randomness',\n",
       "  'remains',\n",
       "  'rise',\n",
       "  'seed',\n",
       "  'show',\n",
       "  'showed',\n",
       "  'size',\n",
       "  'small',\n",
       "  'smaller',\n",
       "  'source',\n",
       "  'strings',\n",
       "  'strong',\n",
       "  'such',\n",
       "  'support',\n",
       "  'trevisan',\n",
       "  'unbalanced',\n",
       "  'use',\n",
       "  'uses',\n",
       "  'using',\n",
       "  'viewed',\n",
       "  'works',\n",
       "  'yields'},\n",
       " 'bag-of-kps': {b'construction',\n",
       "  b'date',\n",
       "  b'domain',\n",
       "  b'explicit extractors',\n",
       "  b'extractors',\n",
       "  b'improved extractors',\n",
       "  b'logk',\n",
       "  b'logn',\n",
       "  b'o',\n",
       "  b'randomness',\n",
       "  b'small',\n",
       "  b'source',\n",
       "  b'strong expansion properties',\n",
       "  b'unbalanced bipartite graph',\n",
       "  b'unbalanced expanders'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_content['index980808']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 02:12:37,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-09-14 02:12:39,527 : INFO : built Dictionary(62388 unique tokens: ['2.5', 'all', 'also', 'analysis', 'approximately']...) from 10000 documents (total 1455421 corpus positions)\n",
      "2018-09-14 02:12:39,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-09-14 02:12:40,231 : INFO : built Dictionary(107954 unique tokens: ['analysis of structured flowcharts', 'cand', 'charts', 'class', 'cn-3/2']...) from 10000 documents (total 289305 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "doc_directory = {}\n",
    "doc_inverse_directory = []\n",
    "texts_tokens = []\n",
    "texts_keyphrases = []\n",
    "for i, (k, doc_content) in enumerate(samples_content.items()):\n",
    "    doc_directory[k] = i\n",
    "    doc_inverse_directory.append(k)\n",
    "    texts_tokens.append(doc_content['tokens'])\n",
    "    texts_keyphrases.append(doc_content['kps-normalized'])\n",
    "\n",
    "dictionary_tokens = corpora.Dictionary(texts_tokens)\n",
    "dictionary_keyphrases = corpora.Dictionary(texts_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = [dictionary_tokens.doc2idx(text) for text in texts_tokens]\n",
    "corpus_keyphrases = [dictionary_keyphrases.doc2idx(text) for text in texts_keyphrases]\n",
    "\n",
    "corpus_bag_of_words = [dictionary_tokens.doc2bow(text) for text in texts_tokens]\n",
    "corpus_bag_of_keyphrases = [dictionary_keyphrases.doc2bow(text) for text in texts_keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 index980808\n",
      "10000 1723\n",
      "\n",
      "Corpus tokens\n",
      "\n",
      "10000 [12809, 29097, 13410, 29100, 29101, 29104, 6785, 576, 29103, 2752, 4984, 153, 2360, 4984, 6740, 29101, 1494, 80, 295, 35, 4984, 1031, 6740, 12809, 29097, 12809, 29096, 1034, 3373, 457, 162, 2655, 316, 1186, 7514, 2297, 7514, 11438, 15294, 21732, 1828, 11438, 35, 654, 440, 372, 33, 6776, 940, 29102, 2579, 523, 254, 1551, 5531, 1297, 12809, 29096, 330, 11839, 254, 29096, 925, 29101, 1031, 356, 1263, 29101, 276, 2970, 3549, 29099, 254, 29101, 1114, 1, 27931, 162, 2655, 7514, 2, 1031, 356, 29098, 999, 1325, 1011, 1762, 297, 2655, 1828, 4316, 221, 162, 2655, 1011, 1762, 1297, 29098, 1551, 330, 4230, 711, 828, 149, 254, 12809, 29096, 3748, 13410, 3682, 2293, 6190, 2595, 105]\n",
      "10000 [(1, 1), (2, 1), (33, 1), (35, 2), (80, 1), (105, 1), (149, 1), (153, 1), (162, 3), (221, 1), (254, 4), (276, 1), (295, 1), (297, 1), (316, 1), (330, 2), (356, 2), (372, 1), (440, 1), (457, 1), (523, 1), (576, 1), (654, 1), (711, 1), (828, 1), (925, 1), (940, 1), (999, 1), (1011, 2), (1031, 3), (1034, 1), (1114, 1), (1186, 1), (1263, 1), (1297, 2), (1325, 1), (1494, 1), (1551, 2), (1762, 2), (1828, 2), (2293, 1), (2297, 1), (2360, 1), (2579, 1), (2595, 1), (2655, 4), (2752, 1), (2970, 1), (3373, 1), (3549, 1), (3682, 1), (3748, 1), (4230, 1), (4316, 1), (4984, 3), (5531, 1), (6190, 1), (6740, 2), (6776, 1), (6785, 1), (7514, 3), (11438, 2), (11839, 1), (12809, 5), (13410, 2), (15294, 1), (21732, 1), (27931, 1), (29096, 4), (29097, 2), (29098, 2), (29099, 1), (29100, 1), (29101, 5), (29102, 1), (29103, 1), (29104, 1)]\n",
      "\n",
      "Corpus keyphrases\n",
      "\n",
      "10000 [27107, 27107, 27101, 1116, 27104, 1341, 552, 548, 103, 27103, 2121, 27102, 25311, 1116, 413, 1116, 1116, 103, 27106, 27105]\n",
      "10000 [(103, 2), (413, 1), (548, 1), (552, 1), (1116, 4), (1341, 1), (2121, 1), (25311, 1), (27101, 1), (27102, 1), (27103, 1), (27104, 1), (27105, 1), (27106, 1), (27107, 2)]\n"
     ]
    }
   ],
   "source": [
    "example = 1723\n",
    "print(len(doc_inverse_directory), doc_inverse_directory[example])\n",
    "print(len(doc_directory), doc_directory[doc_inverse_directory[example]])\n",
    "\n",
    "print(\"\\nCorpus tokens\\n\")\n",
    "print(len(corpus_tokens), corpus_tokens[example])\n",
    "print(len(corpus_bag_of_words), corpus_bag_of_words[example])\n",
    "\n",
    "print(\"\\nCorpus keyphrases\\n\")\n",
    "print(len(corpus_keyphrases), corpus_keyphrases[example])\n",
    "print(len(corpus_bag_of_keyphrases), corpus_bag_of_keyphrases[example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(doc_directory, get_relative_path(data_path, DOC_DIRECTORY))\n",
    "save_pickle(doc_inverse_directory, get_relative_path(data_path, DOC_INVERSE_DIRECTORY))\n",
    "\n",
    "save_pickle(dictionary_tokens, get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "save_pickle(dictionary_keyphrases, get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "\n",
    "save_pickle(corpus_tokens, get_relative_path(data_path, CORPUS_TOKENS))\n",
    "save_pickle(corpus_bag_of_words, get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "save_pickle(corpus_keyphrases, get_relative_path(data_path, CORPUS_KEYPHRASES))\n",
    "save_pickle(corpus_bag_of_keyphrases, get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
