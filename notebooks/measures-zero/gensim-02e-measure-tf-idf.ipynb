{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX))\n",
    "# Load sample B\n",
    "docs_sample_b = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX))\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test: 2111\n",
      "Test: 483\n",
      "10000 62388\n",
      "10000 107954\n"
     ]
    }
   ],
   "source": [
    "doc_directory = load_pickle(get_relative_path(data_path, DOC_DIRECTORY))\n",
    "# Corpus - tokens\n",
    "corpus_bag_of_words = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "dictionary_tokens = load_pickle(get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "# Corpus - keyphrases\n",
    "corpus_bag_of_keyphrases = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))\n",
    "dictionary_keyphrases = load_pickle(get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "\n",
    "print(len(doc_directory))\n",
    "print(\"Test:\", doc_directory[docs_sample_a[0]])\n",
    "print(\"Test:\", doc_directory[docs_sample_b[0]])\n",
    "\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_a[0]]])\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_b[0]]])\n",
    "print(len(corpus_bag_of_words), len(dictionary_tokens))\n",
    "print(len(corpus_bag_of_keyphrases), len(dictionary_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "corpus_a_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_b]\n",
    "corpus_a_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "print(len(corpus_a_tokens), len(corpus_b_tokens), len(corpus_a_keyphrases), len(corpus_b_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-13 17:30:17,999 : INFO : collecting document frequencies\n",
      "2018-09-13 17:30:18,002 : INFO : PROGRESS: processing document #0\n",
      "2018-09-13 17:30:18,413 : INFO : calculating IDF weights for 10000 documents and 62387 features (988950 matrix non-zeros)\n",
      "2018-09-13 17:30:18,627 : INFO : collecting document frequencies\n",
      "2018-09-13 17:30:18,628 : INFO : PROGRESS: processing document #0\n",
      "2018-09-13 17:30:18,732 : INFO : calculating IDF weights for 10000 documents and 107953 features (232171 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "model_tf_idf_tokens = models.TfidfModel(corpus_bag_of_words)\n",
    "model_tf_idf_keyphrases = models.TfidfModel(corpus_bag_of_keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index simmilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-13 17:30:27,536 : INFO : starting similarity index under /tmp/index-tf-idf-tokens\n",
      "/home/snov/environments/artsim/lib/python3.7/site-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-09-13 17:30:32,858 : INFO : creating sparse index\n",
      "2018-09-13 17:30:32,860 : INFO : creating sparse matrix from corpus\n",
      "2018-09-13 17:30:32,864 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-13 17:30:34,656 : INFO : created <5000x62388 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 495130 stored elements in Compressed Sparse Row format>\n",
      "2018-09-13 17:30:34,657 : INFO : creating sparse shard #0\n",
      "2018-09-13 17:30:34,658 : INFO : saving index shard to /tmp/index-tf-idf-tokens.0\n",
      "2018-09-13 17:30:34,659 : INFO : saving SparseMatrixSimilarity object under /tmp/index-tf-idf-tokens.0, separately None\n",
      "2018-09-13 17:30:34,692 : INFO : saved /tmp/index-tf-idf-tokens.0\n",
      "2018-09-13 17:30:34,693 : INFO : loading SparseMatrixSimilarity object from /tmp/index-tf-idf-tokens.0\n",
      "2018-09-13 17:30:34,729 : INFO : loaded /tmp/index-tf-idf-tokens.0\n",
      "2018-09-13 17:30:40,514 : INFO : starting similarity index under /tmp/index-tf-idf-keyphrases\n",
      "2018-09-13 17:30:43,021 : INFO : creating sparse index\n",
      "2018-09-13 17:30:43,023 : INFO : creating sparse matrix from corpus\n",
      "2018-09-13 17:30:43,024 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-13 17:30:43,464 : INFO : created <5000x107954 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 116461 stored elements in Compressed Sparse Row format>\n",
      "2018-09-13 17:30:43,465 : INFO : creating sparse shard #0\n",
      "2018-09-13 17:30:43,466 : INFO : saving index shard to /tmp/index-tf-idf-keyphrases.0\n",
      "2018-09-13 17:30:43,474 : INFO : saving SparseMatrixSimilarity object under /tmp/index-tf-idf-keyphrases.0, separately None\n",
      "2018-09-13 17:30:43,482 : INFO : saved /tmp/index-tf-idf-keyphrases.0\n",
      "2018-09-13 17:30:43,484 : INFO : loading SparseMatrixSimilarity object from /tmp/index-tf-idf-keyphrases.0\n",
      "2018-09-13 17:30:43,494 : INFO : loaded /tmp/index-tf-idf-keyphrases.0\n"
     ]
    }
   ],
   "source": [
    "index_tf_idf_tokens = Similarity(get_tmpfile(\"index-tf-idf-tokens\"), model_tf_idf_tokens[corpus_b_tokens], num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_tf_idf_tokens[model_tf_idf_tokens[corpus_a_tokens]]), get_relative_path(data_path, SIM_TF_IDF_TOKENS))\n",
    "\n",
    "index_tf_idf_keyphrases = Similarity(get_tmpfile(\"index-tf-idf-keyphrases\"), model_tf_idf_keyphrases[corpus_b_keyphrases], num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_idf_keyphrases[model_tf_idf_keyphrases[corpus_a_keyphrases]]), get_relative_path(data_path, SIM_TF_IDF_KEYPHRASES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lsi_tokens = models.LsiModel(corpus_bag_of_words, id2word=dictionary_tokens)\n",
    "# model_lsi_keyphrases = models.TfidfModel(corpus_bag_of_keyphrases)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "index_lsi_tokens = Similarity(get_tmpfile(\"index-lsi-tokens\"), model_lsi_tokens[corpus_b_tokens], num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_lsi_tokens[model_lsi_tokens[corpus_a_tokens]]), get_relative_path(data_path, SIM_LSI_TOKENS))\n",
    "\n",
    "index_lsi_keyphrases = Similarity(get_tmpfile(\"index-lsi-keyphrases\"), model_lsi_keyphrases[corpus_b_keyphrases], num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_lsi_keyphrases[model_lsi_keyphrases[corpus_a_keyphrases]]), get_relative_path(data_path, SIM_LSI_KEYPHRASES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
