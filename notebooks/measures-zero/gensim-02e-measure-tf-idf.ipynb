{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX))\n",
    "# Load sample B\n",
    "docs_sample_b = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX))\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test: 2111\n",
      "Test: 483\n",
      "10000 62388\n",
      "10000 107954\n"
     ]
    }
   ],
   "source": [
    "doc_directory = load_pickle(get_relative_path(data_path, DOC_DIRECTORY))\n",
    "# Corpus - tokens\n",
    "corpus_bag_of_words = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "dictionary_tokens = load_pickle(get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "# Corpus - keyphrases\n",
    "corpus_bag_of_keyphrases = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))\n",
    "dictionary_keyphrases = load_pickle(get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "\n",
    "print(len(doc_directory))\n",
    "print(\"Test:\", doc_directory[docs_sample_a[0]])\n",
    "print(\"Test:\", doc_directory[docs_sample_b[0]])\n",
    "\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_a[0]]])\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_b[0]]])\n",
    "print(len(corpus_bag_of_words), len(dictionary_tokens))\n",
    "print(len(corpus_bag_of_keyphrases), len(dictionary_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "corpus_a_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_b]\n",
    "corpus_a_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "print(len(corpus_a_tokens), len(corpus_b_tokens), len(corpus_a_keyphrases), len(corpus_b_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 02:21:37,150 : INFO : collecting document frequencies\n",
      "2018-09-14 02:21:37,153 : INFO : PROGRESS: processing document #0\n",
      "2018-09-14 02:21:37,493 : INFO : calculating IDF weights for 10000 documents and 62387 features (988950 matrix non-zeros)\n",
      "2018-09-14 02:21:37,688 : INFO : collecting document frequencies\n",
      "2018-09-14 02:21:37,689 : INFO : PROGRESS: processing document #0\n",
      "2018-09-14 02:21:37,787 : INFO : calculating IDF weights for 10000 documents and 107953 features (232171 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "model_tf_idf_tokens = models.TfidfModel(corpus_bag_of_words)\n",
    "model_tf_idf_keyphrases = models.TfidfModel(corpus_bag_of_keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 0.012841377027749524), (36, 0.008727667495260607), (50, 0.028852844980048267), (51, 0.005664177941739807), (80, 0.054138131117306684)]\n",
      "[(105, 0.06695925194076603), (470, 0.07503314787978065), (507, 0.13897025827840198), (566, 0.06990219084546122), (1274, 0.05774539521913921)]\n"
     ]
    }
   ],
   "source": [
    "doc = corpus_bag_of_words[doc_directory[docs_sample_a[0]]]\n",
    "vector1 = model_tf_idf_tokens[doc]\n",
    "\n",
    "doc = corpus_bag_of_keyphrases[doc_directory[docs_sample_a[0]]]\n",
    "vector2 = model_tf_idf_keyphrases[doc]\n",
    "\n",
    "# Example\n",
    "print(vector1[:5])\n",
    "print(vector2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index simmilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 02:21:38,147 : INFO : starting similarity index under /tmp/index-tf-tokens\n",
      "/home/snov/environments/artsim/lib/python3.7/site-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-09-14 02:21:39,789 : INFO : creating sparse index\n",
      "2018-09-14 02:21:39,790 : INFO : creating sparse matrix from corpus\n",
      "2018-09-14 02:21:39,791 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-14 02:21:41,344 : INFO : created <5000x62388 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 495130 stored elements in Compressed Sparse Row format>\n",
      "2018-09-14 02:21:41,345 : INFO : creating sparse shard #0\n",
      "2018-09-14 02:21:41,346 : INFO : saving index shard to /tmp/index-tf-tokens.0\n",
      "2018-09-14 02:21:41,348 : INFO : saving SparseMatrixSimilarity object under /tmp/index-tf-tokens.0, separately None\n",
      "2018-09-14 02:21:41,384 : INFO : saved /tmp/index-tf-tokens.0\n",
      "2018-09-14 02:21:41,385 : INFO : loading SparseMatrixSimilarity object from /tmp/index-tf-tokens.0\n",
      "2018-09-14 02:21:41,407 : INFO : loaded /tmp/index-tf-tokens.0\n",
      "2018-09-14 02:21:43,905 : INFO : starting similarity index under /tmp/index-tf-keyphrases\n",
      "2018-09-14 02:21:45,406 : INFO : creating sparse index\n",
      "2018-09-14 02:21:45,407 : INFO : creating sparse matrix from corpus\n",
      "2018-09-14 02:21:45,408 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-14 02:21:45,841 : INFO : created <5000x107954 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 116461 stored elements in Compressed Sparse Row format>\n",
      "2018-09-14 02:21:45,842 : INFO : creating sparse shard #0\n",
      "2018-09-14 02:21:45,843 : INFO : saving index shard to /tmp/index-tf-keyphrases.0\n",
      "2018-09-14 02:21:45,844 : INFO : saving SparseMatrixSimilarity object under /tmp/index-tf-keyphrases.0, separately None\n",
      "2018-09-14 02:21:45,853 : INFO : saved /tmp/index-tf-keyphrases.0\n",
      "2018-09-14 02:21:45,854 : INFO : loading SparseMatrixSimilarity object from /tmp/index-tf-keyphrases.0\n",
      "2018-09-14 02:21:45,862 : INFO : loaded /tmp/index-tf-keyphrases.0\n"
     ]
    }
   ],
   "source": [
    "index_tf_tokens = Similarity(get_tmpfile(\"index-tf-tokens\"), corpus_b_tokens, num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_tf_tokens[corpus_a_tokens]), get_relative_path(data_path, SIM_TF_TOKENS))\n",
    "\n",
    "index_tf_keyphrases = Similarity(get_tmpfile(\"index-tf-keyphrases\"), corpus_b_keyphrases, num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_keyphrases[corpus_a_keyphrases]), get_relative_path(data_path, SIM_TF_KEYPHRASES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 02:21:46,501 : INFO : starting similarity index under /tmp/index-tf-idf-tokens\n",
      "/home/snov/environments/artsim/lib/python3.7/site-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-09-14 02:21:51,851 : INFO : creating sparse index\n",
      "2018-09-14 02:21:51,852 : INFO : creating sparse matrix from corpus\n",
      "2018-09-14 02:21:51,853 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-14 02:21:53,459 : INFO : created <5000x62388 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 495130 stored elements in Compressed Sparse Row format>\n",
      "2018-09-14 02:21:53,460 : INFO : creating sparse shard #0\n",
      "2018-09-14 02:21:53,462 : INFO : saving index shard to /tmp/index-tf-idf-tokens.0\n",
      "2018-09-14 02:21:53,463 : INFO : saving SparseMatrixSimilarity object under /tmp/index-tf-idf-tokens.0, separately None\n",
      "2018-09-14 02:21:53,497 : INFO : saved /tmp/index-tf-idf-tokens.0\n",
      "2018-09-14 02:21:53,499 : INFO : loading SparseMatrixSimilarity object from /tmp/index-tf-idf-tokens.0\n",
      "2018-09-14 02:21:53,527 : INFO : loaded /tmp/index-tf-idf-tokens.0\n",
      "2018-09-14 02:21:59,556 : INFO : starting similarity index under /tmp/index-tf-idf-keyphrases\n",
      "2018-09-14 02:22:02,327 : INFO : creating sparse index\n",
      "2018-09-14 02:22:02,328 : INFO : creating sparse matrix from corpus\n",
      "2018-09-14 02:22:02,330 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-14 02:22:02,764 : INFO : created <5000x107954 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 116461 stored elements in Compressed Sparse Row format>\n",
      "2018-09-14 02:22:02,765 : INFO : creating sparse shard #0\n",
      "2018-09-14 02:22:02,766 : INFO : saving index shard to /tmp/index-tf-idf-keyphrases.0\n",
      "2018-09-14 02:22:02,767 : INFO : saving SparseMatrixSimilarity object under /tmp/index-tf-idf-keyphrases.0, separately None\n",
      "2018-09-14 02:22:02,778 : INFO : saved /tmp/index-tf-idf-keyphrases.0\n",
      "2018-09-14 02:22:02,780 : INFO : loading SparseMatrixSimilarity object from /tmp/index-tf-idf-keyphrases.0\n",
      "2018-09-14 02:22:02,789 : INFO : loaded /tmp/index-tf-idf-keyphrases.0\n"
     ]
    }
   ],
   "source": [
    "index_tf_idf_tokens = Similarity(get_tmpfile(\"index-tf-idf-tokens\"), model_tf_idf_tokens[corpus_b_tokens], num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_tf_idf_tokens[model_tf_idf_tokens[corpus_a_tokens]]), get_relative_path(data_path, SIM_TF_IDF_TOKENS))\n",
    "\n",
    "index_tf_idf_keyphrases = Similarity(get_tmpfile(\"index-tf-idf-keyphrases\"), model_tf_idf_keyphrases[corpus_b_keyphrases], num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_idf_keyphrases[model_tf_idf_keyphrases[corpus_a_keyphrases]]), get_relative_path(data_path, SIM_TF_IDF_KEYPHRASES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
