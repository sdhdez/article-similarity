{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX))\n",
    "# Load sample B\n",
    "docs_sample_b = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX))\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test: 2111\n",
      "Test: 483\n",
      "10000 62388\n",
      "10000 107954\n"
     ]
    }
   ],
   "source": [
    "doc_directory = load_pickle(get_relative_path(data_path, DOC_DIRECTORY))\n",
    "# Corpus - tokens\n",
    "corpus_bag_of_words = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "corpus_tokens = load_pickle(get_relative_path(data_path, CORPUS_TOKENS))\n",
    "dictionary_tokens = load_pickle(get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "# Corpus - keyphrases\n",
    "corpus_bag_of_keyphrases = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))\n",
    "dictionary_keyphrases = load_pickle(get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "\n",
    "print(len(doc_directory))\n",
    "print(\"Test:\", doc_directory[docs_sample_a[0]])\n",
    "print(\"Test:\", doc_directory[docs_sample_b[0]])\n",
    "\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_a[0]]])\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_b[0]]])\n",
    "print(len(corpus_bag_of_words), len(dictionary_tokens))\n",
    "print(len(corpus_bag_of_keyphrases), len(dictionary_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "corpus_a_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "corpus_a_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "print(len(corpus_a_tokens), len(corpus_b_tokens), len(corpus_a_keyphrases), len(corpus_b_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-19 11:27:05,163 : INFO : collecting document frequencies\n",
      "2018-09-19 11:27:05,165 : INFO : PROGRESS: processing document #0\n",
      "2018-09-19 11:27:05,509 : INFO : calculating IDF weights for 10000 documents and 62387 features (988950 matrix non-zeros)\n",
      "2018-09-19 11:27:05,690 : INFO : collecting document frequencies\n",
      "2018-09-19 11:27:05,691 : INFO : PROGRESS: processing document #0\n",
      "2018-09-19 11:27:05,784 : INFO : calculating IDF weights for 10000 documents and 107953 features (232171 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "model_tf_idf_tokens = models.TfidfModel(corpus_bag_of_words)\n",
    "model_tf_idf_keyphrases = models.TfidfModel(corpus_bag_of_keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 0.012841377027749524), (36, 0.008727667495260607), (50, 0.028852844980048267), (51, 0.005664177941739807), (80, 0.054138131117306684)]\n",
      "[(105, 0.06695925194076603), (470, 0.07503314787978065), (507, 0.13897025827840198), (566, 0.06990219084546122), (1274, 0.05774539521913921)]\n",
      "[(3, 1), (24, 1), (33, 1), (69, 1), (130, 1), (131, 4), (148, 1), (167, 1), (189, 1), (192, 1), (213, 1), (232, 1), (247, 1), (254, 4), (264, 2), (287, 1), (289, 1), (316, 1), (356, 1), (360, 1), (395, 1), (410, 2), (424, 1), (457, 1), (523, 1), (542, 1), (600, 1), (609, 1), (624, 2), (658, 1), (666, 1), (711, 1), (827, 1), (862, 1), (878, 1), (954, 9), (963, 2), (964, 1), (975, 1), (977, 3), (1031, 1), (1154, 1), (1201, 1), (1271, 1), (1394, 1), (1431, 1), (1519, 1), (1556, 1), (1611, 1), (1628, 1), (1663, 4), (1748, 4), (1777, 1), (1819, 1), (1831, 1), (1861, 1), (1867, 1), (2038, 1), (2110, 1), (2113, 1), (2114, 1), (2119, 2), (2269, 3), (2378, 1), (2381, 1), (2579, 1), (2586, 1), (2605, 1), (2735, 2), (2790, 1), (2958, 1), (2969, 1), (3537, 7), (3559, 4), (3842, 1), (4201, 1), (4282, 1), (4555, 1), (5315, 1), (5863, 4), (5864, 2), (6480, 1), (6941, 1), (7638, 1), (9585, 1), (10103, 1), (15267, 1), (15590, 1), (15591, 3), (15592, 1)]\n",
      "[18, 34, 19, 3, 34, 19, 28, 37, 33, 23, 26, 14, 25, 36, 17, 16, 38, 25, 1, 10, 34, 19, 12, 26, 8, 4, 11, 20, 21, 37, 7, 20, 27, 15, 9, 2, 32, 24, 21, 19, 13, 31, 29, 6, 8, 36, 17, 16, 38, 8, 5, 22, 35, 30, 0]\n"
     ]
    }
   ],
   "source": [
    "doc = corpus_bag_of_words[doc_directory[docs_sample_a[0]]]\n",
    "vector1 = model_tf_idf_tokens[doc]\n",
    "\n",
    "doc = corpus_bag_of_keyphrases[doc_directory[docs_sample_a[0]]]\n",
    "vector2 = model_tf_idf_keyphrases[doc]\n",
    "\n",
    "# Example\n",
    "print(vector1[:5])\n",
    "print(vector2[:5])\n",
    "\n",
    "print(corpus_b_tokens[0])\n",
    "print(corpus_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "mat = np.zeros((len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id_a in enumerate(docs_sample_a):\n",
    "    doc_index = doc_directory[id_a]\n",
    "    a = set(corpus_tokens[doc_index])\n",
    "    for j, id_b in enumerate(docs_sample_b):\n",
    "        doc_index = doc_directory[id_b]\n",
    "        b = corpus_tokens[doc_index]\n",
    "        sd = a.symmetric_difference(b)\n",
    "        \n",
    "        mat[i][j] = 1.0 - cosine_distance(v1, v2)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tf_tokens = Similarity(get_tmpfile(\"index-tf-tokens\"), corpus_b_tokens, num_features=len(dictionary_tokens))\n",
    "\n",
    "save_pickle(np.array(index_tf_tokens[corpus_a_tokens]), get_relative_path(data_path, SIM_TF_TOKENS))\n",
    "\n",
    "index_tf_keyphrases = Similarity(get_tmpfile(\"index-tf-keyphrases\"), corpus_b_keyphrases, num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_keyphrases[corpus_a_keyphrases]), get_relative_path(data_path, SIM_TF_KEYPHRASES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tf_idf_tokens = Similarity(get_tmpfile(\"index-tf-idf-tokens\"), model_tf_idf_tokens[corpus_b_tokens], num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_tf_idf_tokens[model_tf_idf_tokens[corpus_a_tokens]]), get_relative_path(data_path, SIM_TF_IDF_TOKENS))\n",
    "\n",
    "index_tf_idf_keyphrases = Similarity(get_tmpfile(\"index-tf-idf-keyphrases\"), model_tf_idf_keyphrases[corpus_b_keyphrases], num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_idf_keyphrases[model_tf_idf_keyphrases[corpus_a_keyphrases]]), get_relative_path(data_path, SIM_TF_IDF_KEYPHRASES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
