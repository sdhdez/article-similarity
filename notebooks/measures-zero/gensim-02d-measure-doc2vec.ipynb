{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX))\n",
    "# Load sample B\n",
    "docs_sample_b = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX))\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test: 2213\n",
      "Test: 483\n",
      "10000 62388\n",
      "10000 107954\n"
     ]
    }
   ],
   "source": [
    "doc_directory = load_pickle(get_relative_path(data_path, DOC_DIRECTORY))\n",
    "\n",
    "# Corpus - tokens\n",
    "dictionary_tokens = load_pickle(get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "corpus_tokens = [[dictionary_tokens[t] for t in doc] for doc in load_pickle(get_relative_path(data_path, CORPUS_TOKENS))]\n",
    "\n",
    "# Corpus - keyphrases\n",
    "dictionary_keyphrases = load_pickle(get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "corpus_keyphrases = [[dictionary_keyphrases[t] for t in doc] for doc in load_pickle(get_relative_path(data_path, CORPUS_KEYPHRASES))]\n",
    "\n",
    "print(len(doc_directory))\n",
    "print(\"Test:\", doc_directory[docs_sample_a[1]])\n",
    "print(\"Test:\", doc_directory[docs_sample_b[0]])\n",
    "\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_a[0]]])\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_b[0]]])\n",
    "print(len(corpus_tokens), len(dictionary_tokens))\n",
    "print(len(corpus_keyphrases), len(dictionary_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: ['variations', 'ocean', 'colour', 'parameters', 'nonuniform', 'vertical', 'profiles', 'chlorophyll', 'concentration', 'based', 'radiative', 'transfer', 'simulations', 'effects', 'nonuniform', 'chlorophyll', 'profiles', 'case', 'waters', 'penetration', 'depth', 'above', 'surface', 'spectral', 'remote', 'sensing', 'reflectance', 'optically', 'weighted', 'chlorophyll', 'concentration', 'investigated', 'simulations', 'nonuniform', 'chlorophyll', 'profiles', 'compared', 'those', 'homogeneous', 'ocean', 'whose', 'chlorophyll', 'concentrations', 'identical', 'surface', 'chlorophyll', 'concentrations', 'inhomogeneous', 'ocean', 'due', 'influence', 'nonuniformity', 'chlorophyll', 'profile', 'maximum', 'relative', 'error', 'penetration', 'depth', '445', 'nm', 'more', 'than', '60', 'spectral', 'remote', 'sensing', 'reflectance', 'about', '40', 'optically', 'weighted', 'chlorophyll', 'concentration', 'about', '40', 'within', 'range', 'our', 'simulations', 'however', 'simulation', 'shows', 'there', 'always', 'spectral', 'band', 'where', 'value', 'above', 'surface', 'remote', 'sensing', 'reflectance', 'influenced', 'nonuniformity', 'depending', 'band', 'new', 'model', 'retrieving', 'sea', 'surface', 'chlorophyll', 'concentration', 'designed', 'adding', 'compensation', 'term', 'into', 'variable', 'seawifs', 'oc2v4', 'algorithm', 'using', 'iterative', 'method', 'new', 'model', 'sea', 'surface', 'chlorophyll', 'concentration', 'well', 'retrieved', 'even', 'area', 'where', 'vertical', 'chlorophyll', 'distribution', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "print(\"Test:\", corpus_tokens[2213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000\n",
      "['variations', 'ocean', 'colour', 'parameters', 'nonuniform', 'vertical', 'profiles', 'chlorophyll', 'concentration', 'based', 'radiative', 'transfer', 'simulations', 'effects', 'nonuniform', 'chlorophyll', 'profiles', 'case', 'waters', 'penetration', 'depth', 'above', 'surface', 'spectral', 'remote', 'sensing', 'reflectance', 'optically', 'weighted', 'chlorophyll', 'concentration', 'investigated', 'simulations', 'nonuniform', 'chlorophyll', 'profiles', 'compared', 'those', 'homogeneous', 'ocean', 'whose', 'chlorophyll', 'concentrations', 'identical', 'surface', 'chlorophyll', 'concentrations', 'inhomogeneous', 'ocean', 'due', 'influence', 'nonuniformity', 'chlorophyll', 'profile', 'maximum', 'relative', 'error', 'penetration', 'depth', '445', 'nm', 'more', 'than', '60', 'spectral', 'remote', 'sensing', 'reflectance', 'about', '40', 'optically', 'weighted', 'chlorophyll', 'concentration', 'about', '40', 'within', 'range', 'our', 'simulations', 'however', 'simulation', 'shows', 'there', 'always', 'spectral', 'band', 'where', 'value', 'above', 'surface', 'remote', 'sensing', 'reflectance', 'influenced', 'nonuniformity', 'depending', 'band', 'new', 'model', 'retrieving', 'sea', 'surface', 'chlorophyll', 'concentration', 'designed', 'adding', 'compensation', 'term', 'into', 'variable', 'seawifs', 'oc2v4', 'algorithm', 'using', 'iterative', 'method', 'new', 'model', 'sea', 'surface', 'chlorophyll', 'concentration', 'well', 'retrieved', 'even', 'area', 'where', 'vertical', 'chlorophyll', 'distribution', 'unknown']\n",
      "['concentration', 'chlorophyll concentration', 'simulations', 'penetration depth', 'above-surface spectral remote-sensing reflectance', 'simulations', 'nonuniform chlorophyll profiles', 'homogeneous ocean', 'surface chlorophyll concentrations', 'inhomogeneous ocean', 'penetration depth', 'spectral remote-sensing reflectance', 'simulations', 'simulation', 'spectral band', 'above-surface remote-sensing reflectance', 'nonuniformity', 'seawifs oc2v4 algorithm', 'area', 'vertical chlorophyll distribution']\n"
     ]
    }
   ],
   "source": [
    "corpus_a_tokens = [corpus_tokens[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_tokens = [corpus_tokens[doc_directory[docid]] for docid in docs_sample_b]\n",
    "corpus_a_keyphrases = [corpus_keyphrases[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_keyphrases = [corpus_keyphrases[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "print(len(corpus_a_tokens), len(corpus_b_tokens), len(corpus_a_keyphrases), len(corpus_b_keyphrases))\n",
    "print(corpus_a_tokens[1])\n",
    "print(corpus_a_keyphrases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_tokens = [models.doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_tokens)] \n",
    "documents_keyphrases = [models.doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_keyphrases)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snov/environments/artsim/lib/python3.7/site-packages/gensim/models/doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2018-09-18 18:21:28,069 : INFO : collecting all words and their counts\n",
      "2018-09-18 18:21:28,071 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-09-18 18:21:28,482 : INFO : collected 62388 word types and 10000 unique tags from a corpus of 10000 examples and 1455421 words\n",
      "2018-09-18 18:21:28,483 : INFO : Loading a fresh vocabulary\n",
      "2018-09-18 18:21:28,756 : INFO : effective_min_count=1 retains 62388 unique words (100% of original 62388, drops 0)\n",
      "2018-09-18 18:21:28,757 : INFO : effective_min_count=1 leaves 1455421 word corpus (100% of original 1455421, drops 0)\n",
      "2018-09-18 18:21:29,119 : INFO : deleting the raw counts dictionary of 62388 items\n",
      "2018-09-18 18:21:29,121 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2018-09-18 18:21:29,122 : INFO : downsampling leaves estimated 1430678 word corpus (98.3% of prior 1455421)\n",
      "2018-09-18 18:21:29,334 : INFO : estimated required memory for 62388 words and 200 dimensions: 139014800 bytes\n",
      "2018-09-18 18:21:29,335 : INFO : resetting layer weights\n",
      "2018-09-18 18:21:30,725 : INFO : training model with 3 workers on 62388 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-09-18 18:21:31,768 : INFO : EPOCH 1 - PROGRESS: at 24.05% examples, 379974 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:32,784 : INFO : EPOCH 1 - PROGRESS: at 54.22% examples, 397498 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:33,797 : INFO : EPOCH 1 - PROGRESS: at 83.42% examples, 403722 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:34,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:34,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:34,291 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:34,293 : INFO : EPOCH - 1 : training on 1455421 raw words (1440708 effective words) took 3.6s, 404367 effective words/s\n",
      "2018-09-18 18:21:35,306 : INFO : EPOCH 2 - PROGRESS: at 24.69% examples, 400700 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:36,312 : INFO : EPOCH 2 - PROGRESS: at 46.82% examples, 356924 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-18 18:21:37,345 : INFO : EPOCH 2 - PROGRESS: at 75.47% examples, 370793 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:38,245 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:38,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:38,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:38,266 : INFO : EPOCH - 2 : training on 1455421 raw words (1440662 effective words) took 4.0s, 363041 effective words/s\n",
      "2018-09-18 18:21:39,270 : INFO : EPOCH 3 - PROGRESS: at 18.81% examples, 315410 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:40,283 : INFO : EPOCH 3 - PROGRESS: at 47.58% examples, 361873 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:41,302 : INFO : EPOCH 3 - PROGRESS: at 76.91% examples, 379179 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:41,956 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:41,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:42,004 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:42,005 : INFO : EPOCH - 3 : training on 1455421 raw words (1440476 effective words) took 3.7s, 385642 effective words/s\n",
      "2018-09-18 18:21:43,017 : INFO : EPOCH 4 - PROGRESS: at 25.29% examples, 413542 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:44,032 : INFO : EPOCH 4 - PROGRESS: at 54.96% examples, 409957 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:45,067 : INFO : EPOCH 4 - PROGRESS: at 84.85% examples, 412230 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:45,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:45,500 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:45,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:45,512 : INFO : EPOCH - 4 : training on 1455421 raw words (1440794 effective words) took 3.5s, 412133 effective words/s\n",
      "2018-09-18 18:21:46,540 : INFO : EPOCH 5 - PROGRESS: at 25.86% examples, 415418 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:47,562 : INFO : EPOCH 5 - PROGRESS: at 55.64% examples, 409631 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-18 18:21:48,573 : INFO : EPOCH 5 - PROGRESS: at 79.05% examples, 386253 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-18 18:21:49,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:49,295 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:49,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:49,301 : INFO : EPOCH - 5 : training on 1455421 raw words (1440787 effective words) took 3.8s, 381076 effective words/s\n",
      "2018-09-18 18:21:49,302 : INFO : training on a 7277105 raw words (7203427 effective words) took 18.6s, 387778 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model_doc2vec_tokens = models.Doc2Vec(documents_tokens,\n",
    "                                      size=200,\n",
    "                                      min_count=1,\n",
    "                                      epochs=5)\n",
    "model_doc2vec_tokens.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(model_doc2vec_tokens.docvecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snov/environments/artsim/lib/python3.7/site-packages/gensim/models/doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2018-09-18 18:21:49,332 : INFO : collecting all words and their counts\n",
      "2018-09-18 18:21:49,334 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-09-18 18:21:49,491 : INFO : collected 107954 word types and 10000 unique tags from a corpus of 10000 examples and 289305 words\n",
      "2018-09-18 18:21:49,492 : INFO : Loading a fresh vocabulary\n",
      "2018-09-18 18:21:49,885 : INFO : effective_min_count=1 retains 107954 unique words (100% of original 107954, drops 0)\n",
      "2018-09-18 18:21:49,886 : INFO : effective_min_count=1 leaves 289305 word corpus (100% of original 289305, drops 0)\n",
      "2018-09-18 18:21:50,297 : INFO : deleting the raw counts dictionary of 107954 items\n",
      "2018-09-18 18:21:50,300 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2018-09-18 18:21:50,301 : INFO : downsampling leaves estimated 288247 word corpus (99.6% of prior 289305)\n",
      "2018-09-18 18:21:50,636 : INFO : estimated required memory for 107954 words and 200 dimensions: 234703400 bytes\n",
      "2018-09-18 18:21:50,638 : INFO : resetting layer weights\n",
      "2018-09-18 18:21:53,185 : INFO : training model with 3 workers on 107954 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-09-18 18:21:54,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:54,271 : INFO : EPOCH 1 - PROGRESS: at 95.22% examples, 268638 words/s, in_qsize 1, out_qsize 1\n",
      "2018-09-18 18:21:54,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:54,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:54,274 : INFO : EPOCH - 1 : training on 289305 raw words (298241 effective words) took 1.1s, 277472 effective words/s\n",
      "2018-09-18 18:21:55,204 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:55,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:55,280 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:55,282 : INFO : EPOCH - 2 : training on 289305 raw words (298242 effective words) took 1.0s, 299181 effective words/s\n",
      "2018-09-18 18:21:56,473 : INFO : EPOCH 3 - PROGRESS: at 83.43% examples, 217490 words/s, in_qsize 4, out_qsize 0\n",
      "2018-09-18 18:21:56,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:56,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:56,632 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:56,633 : INFO : EPOCH - 3 : training on 289305 raw words (298278 effective words) took 1.3s, 222351 effective words/s\n",
      "2018-09-18 18:21:57,659 : INFO : EPOCH 4 - PROGRESS: at 83.43% examples, 252800 words/s, in_qsize 4, out_qsize 0\n",
      "2018-09-18 18:21:57,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:57,767 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:57,779 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:57,780 : INFO : EPOCH - 4 : training on 289305 raw words (298243 effective words) took 1.1s, 262410 effective words/s\n",
      "2018-09-18 18:21:58,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-18 18:21:58,796 : INFO : EPOCH 5 - PROGRESS: at 95.81% examples, 285461 words/s, in_qsize 1, out_qsize 1\n",
      "2018-09-18 18:21:58,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-18 18:21:58,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-18 18:21:58,818 : INFO : EPOCH - 5 : training on 289305 raw words (298260 effective words) took 1.0s, 289342 effective words/s\n",
      "2018-09-18 18:21:58,819 : INFO : training on a 1446525 raw words (1491264 effective words) took 5.6s, 264767 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model_doc2vec_keyphrases = models.Doc2Vec(documents_keyphrases,\n",
    "                                          size=200,\n",
    "                                          min_count=1,\n",
    "                                          epochs=5)\n",
    "model_doc2vec_keyphrases.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.zeros((len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, a in enumerate(docs_sample_a):\n",
    "    doc_index = doc_directory[a]\n",
    "    v1 = model_doc2vec_tokens.docvecs[doc_index]\n",
    "    for j, b in enumerate(docs_sample_b):\n",
    "        doc_index = doc_directory[b]\n",
    "        v2 = model_doc2vec_tokens.docvecs[doc_index]\n",
    "        mat[i][j] = 1.0 - cosine_distance(v1, v2)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_a_keyphrases = [model_doc2vec_keyphrases.docvecs[doc_directory[docid]] for docid in docs_sample_a]\n",
    "doc2vec_b_keyphrases = [model_doc2vec_keyphrases.docvecs[doc_directory[docid]] for docid in docs_sample_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-18 18:23:36,301 : INFO : starting similarity index under /tmp/index-doc2vec-tokens\n",
      "2018-09-18 18:23:36,313 : INFO : creating sparse index\n",
      "2018-09-18 18:23:36,320 : INFO : creating sparse matrix from corpus\n",
      "2018-09-18 18:23:36,323 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-18 18:23:37,062 : INFO : created <5000x62388 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 1000000 stored elements in Compressed Sparse Row format>\n",
      "2018-09-18 18:23:37,063 : INFO : creating sparse shard #0\n",
      "2018-09-18 18:23:37,064 : INFO : saving index shard to /tmp/index-doc2vec-tokens.0\n",
      "2018-09-18 18:23:37,065 : INFO : saving SparseMatrixSimilarity object under /tmp/index-doc2vec-tokens.0, separately None\n",
      "2018-09-18 18:23:37,150 : INFO : saved /tmp/index-doc2vec-tokens.0\n",
      "2018-09-18 18:23:37,151 : INFO : loading SparseMatrixSimilarity object from /tmp/index-doc2vec-tokens.0\n",
      "2018-09-18 18:23:37,209 : INFO : loaded /tmp/index-doc2vec-tokens.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown input type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b60594885809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindex_doc2vec_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_tmpfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index-doc2vec-tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2vec_b_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_doc2vec_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc2vec_a_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_relative_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIM_DOC2VEC_TOKENS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex_doc2vec_keyphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_tmpfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index-doc2vec-keyphrases\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2vec_b_keyphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_keyphrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_doc2vec_keyphrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc2vec_a_keyphrases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_relative_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIM_DOC2VEC_KEYPHRASES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/artsim/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# user asked for all documents => just stack the sub-results into a single matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;31m# (works for both corpus / single doc query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# the following uses a lot of lazy evaluation and (optionally) parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/artsim/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36mquery_shard\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m  \u001b[0;31m# simulate starmap (not part of multiprocessing in older Pythons)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"querying shard %s num_best=%s in process %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished querying shard %s in process %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/artsim/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_best and normalize have to be set before querying a proxy Shard object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/artsim/lib/python3.7/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/artsim/lib/python3.7/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36munitvec\u001b[0;34m(vec, norm, return_norm)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret_normalized_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown input type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown input type"
     ]
    }
   ],
   "source": [
    "save_pickle(np.array(index_doc2vec_tokens[doc2vec_a_tokens]), get_relative_path(data_path, SIM_DOC2VEC_TOKENS))\n",
    "\n",
    "save_pickle(np.array(index_doc2vec_keyphrases[doc2vec_a_keyphrases]), get_relative_path(data_path, SIM_DOC2VEC_KEYPHRASES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_lda_tokens.num_topics)\n",
    "model_lda_tokens.print_topics(num_topics=2, num_words=10)\n",
    "# model_lda_tokens[corpus_tfidf_tokens[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_lda_keyphrases.num_topics)\n",
    "model_lda_keyphrases.print_topics(num_topics=10, num_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
