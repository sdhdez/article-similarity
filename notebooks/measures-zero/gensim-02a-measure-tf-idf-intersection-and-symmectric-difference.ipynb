{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX))\n",
    "# Load sample B\n",
    "docs_sample_b = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX))\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test: 2111\n",
      "Test: 483\n",
      "10000 62388\n",
      "10000 107954\n"
     ]
    }
   ],
   "source": [
    "doc_directory = load_pickle(get_relative_path(data_path, DOC_DIRECTORY))\n",
    "# Corpus - tokens\n",
    "corpus_bag_of_words = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "corpus_tokens = load_pickle(get_relative_path(data_path, CORPUS_TOKENS))\n",
    "dictionary_tokens = load_pickle(get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "# Corpus - keyphrases\n",
    "corpus_bag_of_keyphrases = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))\n",
    "corpus_keyphrases = load_pickle(get_relative_path(data_path, CORPUS_KEYPHRASES))\n",
    "dictionary_keyphrases = load_pickle(get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "\n",
    "print(len(doc_directory))\n",
    "print(\"Test:\", doc_directory[docs_sample_a[0]])\n",
    "print(\"Test:\", doc_directory[docs_sample_b[0]])\n",
    "\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_a[0]]])\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_b[0]]])\n",
    "print(len(corpus_bag_of_words), len(dictionary_tokens))\n",
    "print(len(corpus_bag_of_keyphrases), len(dictionary_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "corpus_a_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_b]\n",
    "corpus_a_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "print(len(corpus_a_tokens), len(corpus_b_tokens), len(corpus_a_keyphrases), len(corpus_b_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-21 16:17:21,493 : INFO : collecting document frequencies\n",
      "2018-09-21 16:17:21,496 : INFO : PROGRESS: processing document #0\n",
      "2018-09-21 16:17:21,833 : INFO : calculating IDF weights for 10000 documents and 62387 features (988950 matrix non-zeros)\n",
      "2018-09-21 16:17:22,025 : INFO : collecting document frequencies\n",
      "2018-09-21 16:17:22,026 : INFO : PROGRESS: processing document #0\n",
      "2018-09-21 16:17:22,128 : INFO : calculating IDF weights for 10000 documents and 107953 features (232171 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "model_tf_idf_tokens = models.TfidfModel(corpus_bag_of_words)\n",
    "model_tf_idf_keyphrases = models.TfidfModel(corpus_bag_of_keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def difference(index_doc1, index_doc2, corpus):\n",
    "    doc1 = set(corpus[index_doc1])\n",
    "    doc2 = set(corpus[index_doc2])\n",
    "    symmdiff = doc1 ^ doc2\n",
    "    doc1_diff = doc1 & symmdiff\n",
    "    doc2_diff = doc2 & symmdiff\n",
    "    return doc1_diff, doc2_diff\n",
    "\n",
    "def filter_bow(index_doc, types_doc, bow):\n",
    "    return list(filter(lambda x: x[0] in types_doc, bow[index_doc]))\n",
    "\n",
    "def difference_bow(index_doc1, index_doc2, corpus, bow):\n",
    "    doc1_diff, doc2_diff = difference(index_doc1, index_doc2, corpus)\n",
    "    doc1_bow = filter_bow(index_doc1, doc1_diff, bow)\n",
    "    doc2_bow = filter_bow(index_doc2, doc2_diff, bow)\n",
    "    return doc1_bow, doc2_bow\n",
    "\n",
    "def intersection(index_doc1, index_doc2, corpus):\n",
    "    doc1 = set(corpus[index_doc1])\n",
    "    doc2 = set(corpus[index_doc2])\n",
    "    intersection = doc1 & doc2\n",
    "    doc1_intersection = doc1 & intersection\n",
    "    doc2_intersection = doc2 & intersection\n",
    "    return doc1_intersection, doc2_intersection\n",
    "\n",
    "def intersection_bow(index_doc1, index_doc2, corpus, bow):\n",
    "    doc1_diff, doc2_diff = intersection(index_doc1, index_doc2, corpus)\n",
    "    doc1_bow = filter_bow(index_doc1, doc1_diff, bow)\n",
    "    doc2_bow = filter_bow(index_doc2, doc2_diff, bow)\n",
    "    return doc1_bow, doc2_bow\n",
    "\n",
    "# types_a_diff, types_b_diff = difference(1952, 6674, corpus_keyphrases)\n",
    "# print(types_a_diff)\n",
    "# print(types_b_diff)\n",
    "# print(filter_bow(1952, types_a_diff, corpus_bag_of_keyphrases))\n",
    "# print(\"++++++++++++++\")\n",
    "\n",
    "\n",
    "\n",
    "def dnorm(dvec):\n",
    "    return np.sqrt(sum(map(lambda x: x[1]**2, dvec)))\n",
    "\n",
    "def ddot(dvec1, dvec2):\n",
    "    d1 = dict(dvec1)\n",
    "    d2 = dict(dvec2)\n",
    "    return sum(d1[key]*d2.get(key, 0.0) for key in d1)\n",
    "\n",
    "def dcosine(dvec1, dvec2):\n",
    "    return ddot(dvec1, dvec2)/(dnorm(dvec1)*dnorm(dvec2))\n",
    "\n",
    "bow_a, bow_b = difference_bow(1000, 3001, corpus_keyphrases, corpus_bag_of_keyphrases)\n",
    "tfidf_a = model_tf_idf_keyphrases[bow_a]\n",
    "tfidf_b = model_tf_idf_keyphrases[bow_b]\n",
    "print(dcosine(tfidf_a, tfidf_b))\n",
    "\n",
    "intersection_a, intersection_b = intersection_bow(1000, 3001, corpus_keyphrases, corpus_bag_of_keyphrases)\n",
    "tfidf_a = model_tf_idf_keyphrases[intersection_a]\n",
    "tfidf_b = model_tf_idf_keyphrases[intersection_b]\n",
    "print(dcosine(tfidf_a, tfidf_b))\n",
    "\n",
    "# print(dcosine(model_tf_idf_keyphrases[intersection_a], model_tf_idf_keyphrases[intersection_b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index simmilarities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "index_tf_tokens = Similarity(get_tmpfile(\"index-tf-tokens\"), corpus_b_tokens, num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_tf_tokens[corpus_a_tokens]), get_relative_path(data_path, SIM_TF_TOKENS))\n",
    "\n",
    "index_tf_keyphrases = Similarity(get_tmpfile(\"index-tf-keyphrases\"), corpus_b_keyphrases, num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_keyphrases[corpus_a_keyphrases]), get_relative_path(data_path, SIM_TF_KEYPHRASES))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "index_tf_idf_tokens = Similarity(get_tmpfile(\"index-tf-idf-tokens\"), model_tf_idf_tokens[corpus_b_tokens], num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_tf_idf_tokens[model_tf_idf_tokens[corpus_a_tokens]]), get_relative_path(data_path, SIM_TF_IDF_TOKENS))\n",
    "\n",
    "index_tf_idf_keyphrases = Similarity(get_tmpfile(\"index-tf-idf-keyphrases\"), model_tf_idf_keyphrases[corpus_b_keyphrases], num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_tf_idf_keyphrases[model_tf_idf_keyphrases[corpus_a_keyphrases]]), get_relative_path(data_path, SIM_TF_IDF_KEYPHRASES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
