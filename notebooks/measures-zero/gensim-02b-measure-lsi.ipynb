{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# Counter\n",
    "from collections import Counter\n",
    "# Package\n",
    "import somhos.resources.dataset as rd\n",
    "import somhos.resources.queries as rq\n",
    "from somhos.methods.useful import save_pickle, load_pickle, wordvectors_centroid\n",
    "from somhos.config.paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_path = \"../../\"\n",
    "data_path = get_relative_path(prefix_path, V9GAMMA_PATH)\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples size: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load sample A\n",
    "docs_sample_a = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_A_SUFFIX))\n",
    "# Load sample B\n",
    "docs_sample_b = load_pickle(get_relative_path(data_path, DOCS_SAMPLE_B_SUFFIX))\n",
    "\n",
    "print(\"Samples size: (%d, %d)\" % (len(docs_sample_a), len(docs_sample_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test: 2111\n",
      "Test: 483\n",
      "10000 62388\n",
      "10000 107954\n"
     ]
    }
   ],
   "source": [
    "doc_directory = load_pickle(get_relative_path(data_path, DOC_DIRECTORY))\n",
    "# Corpus - tokens\n",
    "corpus_bag_of_words = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_WORDS))\n",
    "dictionary_tokens = load_pickle(get_relative_path(data_path, DICTIONARY_TOKENS))\n",
    "# Corpus - keyphrases\n",
    "corpus_bag_of_keyphrases = load_pickle(get_relative_path(data_path, CORPUS_BAG_OF_KEYPHRASES))\n",
    "dictionary_keyphrases = load_pickle(get_relative_path(data_path, DICTIONARY_KEYPHRASES))\n",
    "\n",
    "print(len(doc_directory))\n",
    "print(\"Test:\", doc_directory[docs_sample_a[0]])\n",
    "print(\"Test:\", doc_directory[docs_sample_b[0]])\n",
    "\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_a[0]]])\n",
    "# print(\"Test:\", corpus_bag_of_words[doc_directory[docs_sample_b[0]]])\n",
    "print(len(corpus_bag_of_words), len(dictionary_tokens))\n",
    "print(len(corpus_bag_of_keyphrases), len(dictionary_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "corpus_a_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_tokens = [corpus_bag_of_words[doc_directory[docid]] for docid in docs_sample_b]\n",
    "corpus_a_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_a]\n",
    "corpus_b_keyphrases = [corpus_bag_of_keyphrases[doc_directory[docid]] for docid in docs_sample_b]\n",
    "\n",
    "print(len(corpus_a_tokens), len(corpus_b_tokens), len(corpus_a_keyphrases), len(corpus_b_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 12:58:05,460 : INFO : collecting document frequencies\n",
      "2018-09-14 12:58:05,463 : INFO : PROGRESS: processing document #0\n",
      "2018-09-14 12:58:05,900 : INFO : calculating IDF weights for 10000 documents and 62387 features (988950 matrix non-zeros)\n",
      "2018-09-14 12:58:06,093 : INFO : collecting document frequencies\n",
      "2018-09-14 12:58:06,094 : INFO : PROGRESS: processing document #0\n",
      "2018-09-14 12:58:06,181 : INFO : calculating IDF weights for 10000 documents and 107953 features (232171 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "model_tf_idf_tokens = models.TfidfModel(corpus_bag_of_words)\n",
    "model_tf_idf_keyphrases = models.TfidfModel(corpus_bag_of_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf_tokens = model_tf_idf_tokens[corpus_bag_of_words]\n",
    "corpus_tfidf_keyphrases = model_tf_idf_keyphrases[corpus_bag_of_keyphrases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 12:58:06,542 : INFO : using serial LSI version on this node\n",
      "2018-09-14 12:58:06,544 : INFO : updating model with new documents\n",
      "2018-09-14 12:58:13,536 : INFO : preparing a new chunk of documents\n",
      "2018-09-14 12:58:13,877 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-09-14 12:58:13,877 : INFO : 1st phase: constructing (62388, 300) action matrix\n",
      "2018-09-14 12:58:14,707 : INFO : orthonormalizing (62388, 300) action matrix\n",
      "2018-09-14 12:58:27,320 : INFO : 2nd phase: running dense svd on (300, 10000) matrix\n",
      "2018-09-14 12:58:28,788 : INFO : computing the final decomposition\n",
      "2018-09-14 12:58:28,789 : INFO : keeping 200 factors (discarding 18.533% of energy spectrum)\n",
      "2018-09-14 12:58:33,201 : INFO : processed documents up to #10000\n",
      "2018-09-14 12:58:33,220 : INFO : topic #0(11.229): 0.122*\"data\" + 0.107*\"system\" + 0.107*\"network\" + 0.106*\"model\" + 0.096*\"algorithm\" + 0.091*\"time\" + 0.087*\"systems\" + 0.086*\"design\" + 0.086*\"method\" + 0.085*\"based\"\n",
      "2018-09-14 12:58:33,224 : INFO : topic #1(5.390): 0.280*\"image\" + 0.162*\"images\" + 0.145*\"method\" + -0.144*\"software\" + 0.136*\"algorithm\" + -0.132*\"service\" + -0.131*\"web\" + -0.121*\"management\" + -0.111*\"services\" + -0.103*\"security\"\n",
      "2018-09-14 12:58:33,227 : INFO : topic #2(4.999): -0.267*\"network\" + -0.215*\"networks\" + 0.174*\"image\" + -0.173*\"wireless\" + -0.153*\"routing\" + -0.138*\"channel\" + -0.136*\"power\" + -0.124*\"energy\" + -0.122*\"traffic\" + 0.121*\"learning\"\n",
      "2018-09-14 12:58:33,231 : INFO : topic #3(4.552): -0.333*\"image\" + -0.207*\"video\" + -0.183*\"images\" + -0.171*\"network\" + -0.137*\"mobile\" + -0.127*\"networks\" + -0.111*\"cloud\" + -0.111*\"wireless\" + -0.108*\"detection\" + 0.103*\"design\"\n",
      "2018-09-14 12:58:33,234 : INFO : topic #4(4.343): -0.190*\"graph\" + 0.186*\"power\" + -0.182*\"network\" + -0.170*\"graphs\" + -0.159*\"learning\" + -0.149*\"networks\" + 0.144*\"software\" + 0.135*\"image\" + 0.130*\"design\" + -0.128*\"search\"\n",
      "2018-09-14 12:58:33,298 : INFO : using serial LSI version on this node\n",
      "2018-09-14 12:58:33,299 : INFO : updating model with new documents\n",
      "2018-09-14 12:58:34,985 : INFO : preparing a new chunk of documents\n",
      "2018-09-14 12:58:35,094 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-09-14 12:58:35,095 : INFO : 1st phase: constructing (107954, 300) action matrix\n",
      "2018-09-14 12:58:35,630 : INFO : orthonormalizing (107954, 300) action matrix\n",
      "2018-09-14 12:58:54,416 : INFO : 2nd phase: running dense svd on (300, 10000) matrix\n",
      "2018-09-14 12:58:55,865 : INFO : computing the final decomposition\n",
      "2018-09-14 12:58:55,866 : INFO : keeping 200 factors (discarding 20.992% of energy spectrum)\n",
      "2018-09-14 12:59:03,485 : INFO : processed documents up to #10000\n",
      "2018-09-14 12:59:03,519 : INFO : topic #0(4.343): -0.220*\"algorithm\" + -0.195*\"g\" + -0.192*\"system\" + -0.179*\"algorithms\" + -0.166*\"data\" + -0.162*\"book\" + -0.148*\"simulation\" + -0.142*\"users\" + -0.119*\"solution\" + -0.117*\"information\"\n",
      "2018-09-14 12:59:03,524 : INFO : topic #1(3.843): -0.841*\"g\" + -0.196*\"v\" + -0.123*\"graphs\" + -0.116*\"x\" + -0.113*\"graph\" + -0.108*\"f\" + -0.087*\"k\" + -0.087*\"e\" + 0.076*\"book\" + 0.076*\"system\"\n",
      "2018-09-14 12:59:03,531 : INFO : topic #2(3.178): -0.517*\"book\" + 0.483*\"algorithm\" + 0.223*\"algorithms\" + 0.157*\"o\" + -0.147*\"g\" + 0.106*\"simulation\" + -0.103*\"students\" + -0.096*\"information\" + -0.095*\"tools\" + -0.093*\"users\"\n",
      "2018-09-14 12:59:03,536 : INFO : topic #3(2.960): 0.420*\"book\" + -0.325*\"system\" + 0.299*\"algorithm\" + -0.221*\"g\" + 0.205*\"o\" + -0.205*\"robot\" + 0.165*\"t\" + -0.159*\"simulation\" + 0.144*\"k\" + 0.144*\"c\"\n",
      "2018-09-14 12:59:03,542 : INFO : topic #4(2.834): 0.501*\"classification\" + -0.453*\"robot\" + -0.237*\"simulation\" + -0.190*\"system\" + 0.172*\"algorithms\" + 0.152*\"images\" + 0.141*\"algorithm\" + -0.127*\"t\" + -0.117*\"s\" + 0.114*\"g\"\n"
     ]
    }
   ],
   "source": [
    "model_lsi_tokens = models.LsiModel(corpus_tfidf_tokens, id2word=dictionary_tokens)\n",
    "model_lsi_keyphrases = models.LsiModel(corpus_tfidf_keyphrases, id2word=dictionary_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_a_tokens = model_lsi_tokens[[corpus_tfidf_tokens[doc_directory[docid]] for docid in docs_sample_a]]\n",
    "lsi_b_tokens = model_lsi_tokens[[corpus_tfidf_tokens[doc_directory[docid]] for docid in docs_sample_b]]\n",
    "\n",
    "lsi_a_keyphrases = model_lsi_keyphrases[[corpus_tfidf_keyphrases[doc_directory[docid]] for docid in docs_sample_a]]\n",
    "lsi_b_keyphrases = model_lsi_keyphrases[[corpus_tfidf_keyphrases[doc_directory[docid]] for docid in docs_sample_b]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 12:59:11,883 : INFO : starting similarity index under /tmp/index-lsi-tokens\n",
      "/home/snov/environments/artsim/lib/python3.7/site-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-09-14 12:59:14,403 : INFO : creating sparse index\n",
      "2018-09-14 12:59:14,404 : INFO : creating sparse matrix from corpus\n",
      "2018-09-14 12:59:14,407 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-14 12:59:17,372 : INFO : created <5000x62388 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 1000000 stored elements in Compressed Sparse Row format>\n",
      "2018-09-14 12:59:17,373 : INFO : creating sparse shard #0\n",
      "2018-09-14 12:59:17,374 : INFO : saving index shard to /tmp/index-lsi-tokens.0\n",
      "2018-09-14 12:59:17,376 : INFO : saving SparseMatrixSimilarity object under /tmp/index-lsi-tokens.0, separately None\n",
      "2018-09-14 12:59:17,449 : INFO : saved /tmp/index-lsi-tokens.0\n",
      "2018-09-14 12:59:17,450 : INFO : loading SparseMatrixSimilarity object from /tmp/index-lsi-tokens.0\n",
      "2018-09-14 12:59:17,498 : INFO : loaded /tmp/index-lsi-tokens.0\n",
      "2018-09-14 12:59:38,805 : INFO : starting similarity index under /tmp/index-lsi-keyphrases\n",
      "2018-09-14 12:59:41,062 : INFO : creating sparse index\n",
      "2018-09-14 12:59:41,063 : INFO : creating sparse matrix from corpus\n",
      "2018-09-14 12:59:41,064 : INFO : PROGRESS: at document #0/5000\n",
      "2018-09-14 12:59:44,038 : INFO : created <5000x107954 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 1000000 stored elements in Compressed Sparse Row format>\n",
      "2018-09-14 12:59:44,039 : INFO : creating sparse shard #0\n",
      "2018-09-14 12:59:44,040 : INFO : saving index shard to /tmp/index-lsi-keyphrases.0\n",
      "2018-09-14 12:59:44,041 : INFO : saving SparseMatrixSimilarity object under /tmp/index-lsi-keyphrases.0, separately None\n",
      "2018-09-14 12:59:44,107 : INFO : saved /tmp/index-lsi-keyphrases.0\n",
      "2018-09-14 12:59:44,108 : INFO : loading SparseMatrixSimilarity object from /tmp/index-lsi-keyphrases.0\n",
      "2018-09-14 12:59:44,161 : INFO : loaded /tmp/index-lsi-keyphrases.0\n"
     ]
    }
   ],
   "source": [
    "index_lsi_tokens = Similarity(get_tmpfile(\"index-lsi-tokens\"), lsi_b_tokens, num_features=len(dictionary_tokens))\n",
    "save_pickle(np.array(index_lsi_tokens[lsi_a_tokens]), get_relative_path(data_path, SIM_LSI_TOKENS))\n",
    "\n",
    "index_lsi_keyphrases = Similarity(get_tmpfile(\"index-lsi-keyphrases\"), lsi_b_keyphrases, num_features=len(dictionary_keyphrases))\n",
    "save_pickle(np.array(index_lsi_keyphrases[lsi_a_keyphrases]), get_relative_path(data_path, SIM_LSI_KEYPHRASES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 13:00:04,757 : INFO : topic #0(11.229): 0.122*\"data\" + 0.107*\"system\" + 0.107*\"network\" + 0.106*\"model\" + 0.096*\"algorithm\" + 0.091*\"time\" + 0.087*\"systems\" + 0.086*\"design\" + 0.086*\"method\" + 0.085*\"based\"\n",
      "2018-09-14 13:00:04,761 : INFO : topic #1(5.390): 0.280*\"image\" + 0.162*\"images\" + 0.145*\"method\" + -0.144*\"software\" + 0.136*\"algorithm\" + -0.132*\"service\" + -0.131*\"web\" + -0.121*\"management\" + -0.111*\"services\" + -0.103*\"security\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.122*\"data\" + 0.107*\"system\" + 0.107*\"network\" + 0.106*\"model\" + 0.096*\"algorithm\" + 0.091*\"time\" + 0.087*\"systems\" + 0.086*\"design\" + 0.086*\"method\" + 0.085*\"based\"'),\n",
       " (1,\n",
       "  '0.280*\"image\" + 0.162*\"images\" + 0.145*\"method\" + -0.144*\"software\" + 0.136*\"algorithm\" + -0.132*\"service\" + -0.131*\"web\" + -0.121*\"management\" + -0.111*\"services\" + -0.103*\"security\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_lsi_tokens.num_topics)\n",
    "model_lsi_tokens.print_topics(num_topics=2, num_words=10)\n",
    "# model_lsi_tokens[corpus_tfidf_tokens[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-14 15:07:46,286 : INFO : topic #0(4.343): -0.220*\"algorithm\" + -0.195*\"g\" + -0.192*\"system\" + -0.179*\"algorithms\" + -0.166*\"data\" + -0.162*\"book\" + -0.148*\"simulation\" + -0.142*\"users\" + -0.119*\"solution\" + -0.117*\"information\"\n",
      "2018-09-14 15:07:46,292 : INFO : topic #1(3.843): -0.841*\"g\" + -0.196*\"v\" + -0.123*\"graphs\" + -0.116*\"x\" + -0.113*\"graph\" + -0.108*\"f\" + -0.087*\"k\" + -0.087*\"e\" + 0.076*\"book\" + 0.076*\"system\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.220*\"algorithm\" + -0.195*\"g\" + -0.192*\"system\" + -0.179*\"algorithms\" + -0.166*\"data\" + -0.162*\"book\" + -0.148*\"simulation\" + -0.142*\"users\" + -0.119*\"solution\" + -0.117*\"information\"'),\n",
       " (1,\n",
       "  '-0.841*\"g\" + -0.196*\"v\" + -0.123*\"graphs\" + -0.116*\"x\" + -0.113*\"graph\" + -0.108*\"f\" + -0.087*\"k\" + -0.087*\"e\" + 0.076*\"book\" + 0.076*\"system\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_lsi_keyphrases.num_topics)\n",
    "model_lsi_keyphrases.print_topics(num_topics=2, num_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
